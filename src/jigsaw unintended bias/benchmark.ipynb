{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import sys\n",
    "# this adds the src folder in the sys path, where the metric_utils.py file is\n",
    "# not needed if this notebook is in the same folder, but uncomment to access from the data subfolders\n",
    "sys.path.append( '..' )\n",
    "from metric_utils import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'jigsaw unintended bias'\n",
    "model_name = 'bert'\n",
    "\n",
    "# change to f'../../results/{dataset_name}' when using inside one of the data subfolders\n",
    "result_folder = f'../../results/{dataset_name}'\n",
    "test_csv_filepath = os.path.join(result_folder, 'test.csv')\n",
    "\n",
    "model_folder = os.path.join(result_folder, model_name) # for this particular model\n",
    "normal_folder = os.path.join(model_folder, 'normal')\n",
    "dp_folder = os.path.join(model_folder, 'epsilon 3.0')\n",
    "\n",
    "result_filepath = os.path.join(normal_folder, 'results.csv')\n",
    "dp_result_filepath = os.path.join(dp_folder, 'results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset can be preprocessed from the original dataset to be used here. To simplify things, I saved the preprocessed datasets during the tokenizing process as csv files and then downloaded them in the corresponding dataset folder of [`result`](../results/) directory.\n",
    "\n",
    "You can recreate the processed datasets using the tokenize notebooks for that particular dataset. That would give you train, test and validation csv files as well as the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7084460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7141509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  male  female  white  black\n",
       "0  7084460   NaN     NaN    NaN    NaN\n",
       "1  7141509   NaN     NaN    NaN    NaN"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv(result_filepath)\n",
    "dp_result = pd.read_csv(dp_result_filepath)\n",
    "\n",
    "df = pd.read_csv(test_csv_filepath)\n",
    "# df.drop(columns=['comment_text', 'labels'], inplace=True)\n",
    "# df.to_csv(test_csv_filepath, index=False)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>probs</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>294256</td>\n",
       "      <td>0</td>\n",
       "      <td>0.234011</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>434688</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055698</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  labels     probs  split\n",
       "0  294256       0  0.234011  train\n",
       "1  434688       0  0.055698  train"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>probs</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7104596</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023603</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7136711</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  labels     probs split\n",
       "0  7104596       0  0.023603  test\n",
       "1  7136711       0  0.007524  test"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only calculate test result\n",
    "result = result[result['split']=='test']\n",
    "dp_result = dp_result[dp_result['split']=='test']\n",
    "\n",
    "# drop split column\n",
    "result.drop(columns=['split'], inplace=True)\n",
    "dp_result.drop(columns=['split'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = result.drop(columns='id').reset_index().rename({'index':'id'}, axis=1)\n",
    "# dp_result = dp_result.drop(columns='id').reset_index().rename({'index':'id'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary fix for invalid indexing in the result file\n",
    "for column in df.columns:\n",
    "    if column =='id': continue\n",
    "    result[column]  = df[df['id'].isin(dp_result['id'])][column].values\n",
    "\n",
    "# result = result.merge(df, on=id_column, how='inner').reset_index(drop=True)\n",
    "dp_result = dp_result.merge(df, on=id_column, how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert probability to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[prediction_column] = result[probability_column] >=0.5\n",
    "dp_result[prediction_column] = dp_result[probability_column] >=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'male', 'black', 'white']\n"
     ]
    }
   ],
   "source": [
    "group_map = {\n",
    "    'gender': {\n",
    "        'unprivileged':['female'],\n",
    "        'privileged':['male']\n",
    "    },\n",
    "    'race': {\n",
    "        'unprivileged':['black'],\n",
    "        'privileged': ['white']\n",
    "    }\n",
    "}\n",
    "\n",
    "identities = []\n",
    "for group_key in group_map.keys():\n",
    "    subgroup_map = group_map[group_key]\n",
    "    for subgroup_key in subgroup_map.keys():\n",
    "        identities.extend(subgroup_map[subgroup_key])\n",
    "\n",
    "print(identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize identity and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = binarize(result, [target_column] + identities)\n",
    "dp_result = binarize(dp_result, [target_column] + identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fairness_metrics</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender_DP</th>\n",
       "      <th>race</th>\n",
       "      <th>race_DP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demographic parity</td>\n",
       "      <td>0.973485</td>\n",
       "      <td>0.975852</td>\n",
       "      <td>0.948217</td>\n",
       "      <td>0.797963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Equality of Opportunity (w.r.t y = 1)</td>\n",
       "      <td>0.976655</td>\n",
       "      <td>0.978125</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Equality of Opportunity (w.r.t y = 0)</td>\n",
       "      <td>0.991862</td>\n",
       "      <td>0.980068</td>\n",
       "      <td>0.993239</td>\n",
       "      <td>0.938017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Equality of Odds</td>\n",
       "      <td>0.984258</td>\n",
       "      <td>0.979097</td>\n",
       "      <td>0.946620</td>\n",
       "      <td>0.920706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unprotected-accuracy</td>\n",
       "      <td>0.925058</td>\n",
       "      <td>0.822829</td>\n",
       "      <td>0.930355</td>\n",
       "      <td>0.712221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>protected-accuracy</td>\n",
       "      <td>0.917614</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>0.699491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.921336</td>\n",
       "      <td>0.814823</td>\n",
       "      <td>0.921884</td>\n",
       "      <td>0.705856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        fairness_metrics    gender  gender_DP      race  \\\n",
       "0                     demographic parity  0.973485   0.975852  0.948217   \n",
       "1  Equality of Opportunity (w.r.t y = 1)  0.976655   0.978125  0.900000   \n",
       "2  Equality of Opportunity (w.r.t y = 0)  0.991862   0.980068  0.993239   \n",
       "3                       Equality of Odds  0.984258   0.979097  0.946620   \n",
       "4                   unprotected-accuracy  0.925058   0.822829  0.930355   \n",
       "5                     protected-accuracy  0.917614   0.806818  0.913413   \n",
       "6                               accuracy  0.921336   0.814823  0.921884   \n",
       "\n",
       "    race_DP  \n",
       "0  0.797963  \n",
       "1  0.903395  \n",
       "2  0.938017  \n",
       "3  0.920706  \n",
       "4  0.712221  \n",
       "5  0.699491  \n",
       "6  0.705856  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_results = {\n",
    "   # make sure your calculate bias method returns bias metrics in the same order\n",
    "   'fairness_metrics': ['demographic parity', 'Equality of Opportunity (w.r.t y = 1)',\n",
    "'Equality of Opportunity (w.r.t y = 0)', 'Equality of Odds', 'unprotected-accuracy',\n",
    "'protected-accuracy', 'accuracy']\n",
    "}\n",
    "\n",
    "for group_key in group_map.keys():\n",
    "   subgroup_map = group_map[group_key]\n",
    "   privileged_group = subgroup_map['privileged']\n",
    "   unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "   bias_results[group_key] = calculate_bias(result, privileged_group, unprivileged_group)\n",
    "   bias_results[group_key+'_DP'] = calculate_bias(dp_result, privileged_group, unprivileged_group)\n",
    "\n",
    "bias_results = pd.DataFrame(bias_results) \n",
    "bias_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_results.round(3).to_csv(os.path.join(dp_folder, 'bias.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>male</th>\n",
       "      <th>male_DP</th>\n",
       "      <th>female</th>\n",
       "      <th>female_DP</th>\n",
       "      <th>white</th>\n",
       "      <th>white_DP</th>\n",
       "      <th>black</th>\n",
       "      <th>black_DP</th>\n",
       "      <th>Total</th>\n",
       "      <th>Total_DP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.942255</td>\n",
       "      <td>0.849390</td>\n",
       "      <td>0.948792</td>\n",
       "      <td>0.850251</td>\n",
       "      <td>0.940519</td>\n",
       "      <td>0.771067</td>\n",
       "      <td>0.960141</td>\n",
       "      <td>0.737525</td>\n",
       "      <td>0.941883</td>\n",
       "      <td>0.894798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.917614</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.925058</td>\n",
       "      <td>0.822829</td>\n",
       "      <td>0.913413</td>\n",
       "      <td>0.699491</td>\n",
       "      <td>0.930355</td>\n",
       "      <td>0.712221</td>\n",
       "      <td>0.925390</td>\n",
       "      <td>0.909094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.531034</td>\n",
       "      <td>0.618395</td>\n",
       "      <td>0.515247</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.602247</td>\n",
       "      <td>0.607407</td>\n",
       "      <td>0.598165</td>\n",
       "      <td>0.614494</td>\n",
       "      <td>0.553024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>0.407654</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>0.499069</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.545151</td>\n",
       "      <td>0.523332</td>\n",
       "      <td>0.455476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.721875</td>\n",
       "      <td>0.734884</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.759207</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.662602</td>\n",
       "      <td>0.744117</td>\n",
       "      <td>0.703742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>false positive rate</td>\n",
       "      <td>0.065951</td>\n",
       "      <td>0.178013</td>\n",
       "      <td>0.057813</td>\n",
       "      <td>0.158082</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>0.326061</td>\n",
       "      <td>0.061885</td>\n",
       "      <td>0.264078</td>\n",
       "      <td>0.058866</td>\n",
       "      <td>0.073071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               metrics      male   male_DP    female  female_DP     white  \\\n",
       "0                  auc  0.942255  0.849390  0.948792   0.850251  0.940519   \n",
       "1             accuracy  0.917614  0.806818  0.925058   0.822829  0.913413   \n",
       "2             f1_score  0.560606  0.531034  0.618395   0.515247  0.585366   \n",
       "3            precision  0.462500  0.420000  0.533784   0.407654  0.493151   \n",
       "4               recall  0.711538  0.721875  0.734884   0.700000  0.720000   \n",
       "5  false positive rate  0.065951  0.178013  0.057813   0.158082  0.068646   \n",
       "\n",
       "   white_DP     black  black_DP     Total  Total_DP  \n",
       "0  0.771067  0.960141  0.737525  0.941883  0.894798  \n",
       "1  0.699491  0.930355  0.712221  0.925390  0.909094  \n",
       "2  0.602247  0.607407  0.598165  0.614494  0.553024  \n",
       "3  0.499069  0.482353  0.545151  0.523332  0.455476  \n",
       "4  0.759207  0.820000  0.662602  0.744117  0.703742  \n",
       "5  0.326061  0.061885  0.264078  0.058866  0.073071  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results = {\n",
    "    'metrics': ['auc', 'accuracy', 'f1_score', 'precision', 'recall', 'false positive rate']\n",
    "}\n",
    "\n",
    "for group_key in group_map.keys():\n",
    "    subgroup_map = group_map[group_key]\n",
    "    privileged_group = subgroup_map['privileged']\n",
    "    unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "    privileged_group_name = ','.join(privileged_group)\n",
    "    unprivileged_group_name = ','.join(unprivileged_group)\n",
    "\n",
    "    overall_results[privileged_group_name] = calculate_metrics(result, privileged_group)\n",
    "    overall_results[privileged_group_name + '_DP'] = calculate_metrics(dp_result, privileged_group)\n",
    "\n",
    "    overall_results[unprivileged_group_name] = calculate_metrics(result, unprivileged_group)\n",
    "    overall_results[unprivileged_group_name + '_DP'] = calculate_metrics(dp_result, unprivileged_group)\n",
    "\n",
    "overall_results['Total'] = calculate_metrics(result, [])\n",
    "overall_results['Total_DP'] = calculate_metrics(dp_result, [])\n",
    "\n",
    "overall_results = pd.DataFrame(overall_results) \n",
    "overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results.round(3).to_csv(os.path.join(dp_folder, 'overall_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch benchmarking\n",
    "Benchmark all models and privacy budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['bert']:\n",
    "    model_folder = os.path.join(result_folder, model_name) # for this particular model\n",
    "    normal_folder = os.path.join(model_folder, 'normal')\n",
    "    result_filepath = os.path.join(normal_folder, 'results.csv')\n",
    "\n",
    "    result = pd.read_csv(result_filepath)\n",
    "    result = result[result['split']=='test']\n",
    "    # drop split column\n",
    "    result.drop(columns=['split'], inplace=True)\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column =='id': continue\n",
    "        result[column]  = df[df['id'].isin(dp_result['id'])][column].values\n",
    "\n",
    "    result[prediction_column] = result[probability_column] >=0.5\n",
    "    result = binarize(result, [target_column] + identities)\n",
    "\n",
    "    \n",
    "    for epsilon in [3.0, 6.0, 9.0]:\n",
    "        dp_folder = os.path.join(model_folder, f'epsilon {epsilon}')\n",
    "        dp_result_filepath = os.path.join(dp_folder, 'results.csv')\n",
    "        dp_result = pd.read_csv(dp_result_filepath)\n",
    "\n",
    "        # only calculate test result\n",
    "        dp_result = dp_result[dp_result['split']=='test']\n",
    "        dp_result.drop(columns=['split'], inplace=True)\n",
    "        dp_result = dp_result.merge(df, on=id_column, how='inner').reset_index(drop=True)\n",
    "        \n",
    "        dp_result[prediction_column] = dp_result[probability_column] >=0.5\n",
    "        dp_result = binarize(dp_result, [target_column] + identities)\n",
    "\n",
    "        bias_results = {\n",
    "        'fairness_metrics': ['demographic parity', 'Equality of Opportunity (w.r.t y = 1)',\n",
    "        'Equality of Opportunity (w.r.t y = 0)', 'Equality of Odds', 'unprotected-accuracy',\n",
    "        'protected-accuracy', 'accuracy']\n",
    "        }\n",
    "\n",
    "        for group_key in group_map.keys():\n",
    "            subgroup_map = group_map[group_key]\n",
    "            privileged_group = subgroup_map['privileged']\n",
    "            unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "            bias_results[group_key] = calculate_bias(result, privileged_group, unprivileged_group)\n",
    "            bias_results[group_key+'_DP'] = calculate_bias(dp_result, privileged_group, unprivileged_group)\n",
    "\n",
    "        bias_results = pd.DataFrame(bias_results) \n",
    "        bias_results.round(3).to_csv(os.path.join(dp_folder, 'bias.csv'), index=False)\n",
    "\n",
    "\n",
    "        overall_results = {\n",
    "            'metrics': ['auc', 'accuracy', 'f1_score', 'precision', 'recall', 'false positive rate']\n",
    "        }\n",
    "\n",
    "        for group_key in group_map.keys():\n",
    "            subgroup_map = group_map[group_key]\n",
    "            privileged_group = subgroup_map['privileged']\n",
    "            unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "            privileged_group_name = ','.join(privileged_group)\n",
    "            unprivileged_group_name = ','.join(unprivileged_group)\n",
    "\n",
    "            overall_results[privileged_group_name] = calculate_metrics(result, privileged_group)\n",
    "            overall_results[privileged_group_name + '_DP'] = calculate_metrics(dp_result, privileged_group)\n",
    "\n",
    "            overall_results[unprivileged_group_name] = calculate_metrics(result, unprivileged_group)\n",
    "            overall_results[unprivileged_group_name + '_DP'] = calculate_metrics(dp_result, unprivileged_group)\n",
    "\n",
    "        overall_results['Total'] = calculate_metrics(result, [])\n",
    "        overall_results['Total_DP'] = calculate_metrics(dp_result, [])\n",
    "\n",
    "        overall_results = pd.DataFrame(overall_results) \n",
    "        overall_results.columns = [col.replace('target_', '') for col in overall_results.columns]\n",
    "        overall_results.round(3).to_csv(os.path.join(dp_folder, 'overall_results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is done following \n* [Building text classifier with Differential Privacy](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)\n* [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.4.0/custom_datasets.html#seq-imdb)","metadata":{"id":"huyLXVRDUwFw"}},{"cell_type":"markdown","source":"# Libraries\nhttps://huggingface.co/docs/transformers/training","metadata":{"id":"n3CQPh6pRJpP"}},{"cell_type":"markdown","source":"## Install","metadata":{"id":"tB5WsXAHyBZv"}},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-07T22:35:32.588760Z","iopub.execute_input":"2022-05-07T22:35:32.589148Z","iopub.status.idle":"2022-05-07T22:35:46.122057Z","shell.execute_reply.started":"2022-05-07T22:35:32.589057Z","shell.execute_reply":"2022-05-07T22:35:46.121206Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{"id":"nxhRYG1UyFLJ"}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import DataLoader\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\npd.set_option('display.max_columns', None)","metadata":{"id":"MjZvbCbYx-no","execution":{"iopub.status.busy":"2022-05-07T22:36:29.231313Z","iopub.execute_input":"2022-05-07T22:36:29.232025Z","iopub.status.idle":"2022-05-07T22:36:29.237032Z","shell.execute_reply.started":"2022-05-07T22:36:29.231985Z","shell.execute_reply":"2022-05-07T22:36:29.236286Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T22:36:31.756183Z","iopub.execute_input":"2022-05-07T22:36:31.756895Z","iopub.status.idle":"2022-05-07T22:36:31.768029Z","shell.execute_reply.started":"2022-05-07T22:36:31.756856Z","shell.execute_reply":"2022-05-07T22:36:31.767057Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef seed_torch(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\nglobal_seed = 2022\nseed_torch(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T22:36:33.539655Z","iopub.execute_input":"2022-05-07T22:36:33.540199Z","iopub.status.idle":"2022-05-07T22:36:33.548341Z","shell.execute_reply.started":"2022-05-07T22:36:33.540159Z","shell.execute_reply":"2022-05-07T22:36:33.547542Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Get device","metadata":{"id":"CSnXCK2k6KvV"}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"id":"N01plY896Mps","outputId":"3d0d33d9-0b02-4ad8-c8c0-b227b57bf1fb","execution":{"iopub.status.busy":"2022-05-07T22:36:35.611862Z","iopub.execute_input":"2022-05-07T22:36:35.612125Z","iopub.status.idle":"2022-05-07T22:36:35.675329Z","shell.execute_reply.started":"2022-05-07T22:36:35.612097Z","shell.execute_reply":"2022-05-07T22:36:35.674529Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load tokenized data\n\nFrom my [other notebook](https://www.kaggle.com/code/khairulislam/tokenize-jigsaw-comments). The dataset is tokenized from the [Jigsaw competition]( https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) and [all_data.csv](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv)","metadata":{}},{"cell_type":"code","source":"text = 'comment_text'\ntarget = 'labels'\nroot = '/kaggle/input/tokenize-jigsaw-comments/'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T22:36:38.156280Z","iopub.execute_input":"2022-05-07T22:36:38.156886Z","iopub.status.idle":"2022-05-07T22:36:38.160881Z","shell.execute_reply.started":"2022-05-07T22:36:38.156847Z","shell.execute_reply":"2022-05-07T22:36:38.160062Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import pickle\n    \nwith open(root + 'test.pkl', 'rb') as input_file:\n    test_all_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'train_undersampled.pkl', 'rb') as input_file:\n    train_all_tokenized = pickle.load(input_file)\n    input_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T22:36:40.099816Z","iopub.execute_input":"2022-05-07T22:36:40.100285Z","iopub.status.idle":"2022-05-07T22:36:42.918964Z","shell.execute_reply.started":"2022-05-07T22:36:40.100246Z","shell.execute_reply":"2022-05-07T22:36:42.918181Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_tokenized = train_all_tokenized.remove_columns(['id'])\ntest_tokenized = test_all_tokenized.remove_columns(['id'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T22:36:42.972254Z","iopub.execute_input":"2022-05-07T22:36:42.972919Z","iopub.status.idle":"2022-05-07T22:36:42.984189Z","shell.execute_reply.started":"2022-05-07T22:36:42.972882Z","shell.execute_reply":"2022-05-07T22:36:42.983409Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nBERT (Bidirectional Encoder Representations from Transformers) is a state of the art approach to various NLP tasks. It uses a Transformer architecture and relies heavily on the concept of pre-training.\n\nWe'll use a pre-trained BERT-base model, provided in huggingface [transformers](https://github.com/huggingface/transformers) repo. It gives us a pytorch implementation for the classic BERT architecture, as well as a tokenizer and weights pre-trained on a public English corpus (Wikipedia).\n\nPlease follow these [installation instrucitons](https://github.com/huggingface/transformers#installation) before proceeding.","metadata":{"id":"smey0wVLOG3Q"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_pretrained_model(model_name, num_labels):\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n    trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n    total_params = 0\n    trainable_params = 0\n\n    for p in model.parameters():\n        p.requires_grad = False\n        total_params += p.numel()\n\n    for layer in trainable_layers:\n        for p in layer.parameters():\n            p.requires_grad = True\n            trainable_params += p.numel()\n\n    print(f\"Total parameters count: {total_params}\") # ~108M\n    print(f\"Trainable parameters count: {trainable_params}\") # ~7M\n\n    return model","metadata":{"id":"XtuQ-VCYOJM_","execution":{"iopub.status.busy":"2022-05-07T22:36:46.123487Z","iopub.execute_input":"2022-05-07T22:36:46.124164Z","iopub.status.idle":"2022-05-07T22:36:46.130121Z","shell.execute_reply.started":"2022-05-07T22:36:46.124129Z","shell.execute_reply":"2022-05-07T22:36:46.129454Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"num_labels = 2\n# model_name = \"bert-base-uncased\"\nmodel_name = 'prajjwal1/bert-small'","metadata":{"id":"dfJ3Mun0L4uR","outputId":"5891754e-9c58-41a3-ec1a-64fddca917d1","execution":{"iopub.status.busy":"2022-05-07T22:36:48.932845Z","iopub.execute_input":"2022-05-07T22:36:48.933448Z","iopub.status.idle":"2022-05-07T22:37:04.409539Z","shell.execute_reply.started":"2022-05-07T22:36:48.933403Z","shell.execute_reply":"2022-05-07T22:37:04.408765Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"RMuUO5vdN2Ux"}},{"cell_type":"markdown","source":"## Utils","metadata":{"id":"ZPvOGP6ZN9TD"}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nsigmoid = torch.nn.Sigmoid()\n\n# https://huggingface.co/docs/datasets/metrics\ndef calculate_result(labels, probs, threshold=0.5):\n    preds = np.where(probs >= threshold, 1, 0)\n    return {\n        'accuracy': np.round(accuracy_score(labels, preds), 4),\n        'f1': np.round(f1_score(labels, preds), 4),\n        'auc': np.round(roc_auc_score(labels, probs), 4)\n    }\n\ndef evaluate(model, test_dataloader, epoch):    \n    model.eval()\n\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n    progress_bar = tqdm(range(len(test_dataloader)), desc=f'Epoch {epoch} (Test)')\n    \n    for batch in test_dataloader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        \n        probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n        labels = inputs[target].detach().cpu().numpy()\n        \n        losses.append(loss.item())\n        total_probs = torch.cat((total_probs, probs), dim=0)\n        total_labels.extend(labels)\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(\n            loss=np.round(np.mean(losses), 4), \n            f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n        )\n    \n    model.train()\n    test_result = calculate_result(total_labels, total_probs)\n    return np.mean(losses), test_result, total_probs\n\ndef train(model, train_dataloader, epoch):\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n    progress_bar = tqdm(range(len(train_dataloader)), desc=f'Epoch {epoch} (Train)')\n\n    for step, data in enumerate(train_dataloader):\n        optimizer.zero_grad()\n\n        inputs = {k: v.to(device) for k, v in data.items()}\n        outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n\n        # targets = data[target].to(device, dtype = torch.long)\n        # loss = loss_function(outputs.logits, targets)\n        loss = outputs[0]\n\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n        # preds = np.argmax(outputs.logits.detach().cpu().numpy(), axis=1)\n        probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n        labels = inputs[target].detach().cpu().numpy()\n        \n        total_probs = torch.cat((total_probs, probs), dim=0)\n        total_labels.extend(labels)\n\n        progress_bar.update(1)\n        progress_bar.set_postfix(\n            loss=np.round(np.mean(losses), 4), \n            f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n        )\n\n\n    train_loss = np.mean(losses)\n    train_result = calculate_result(np.array(total_labels), np.array(total_probs))\n\n    return train_loss, train_result, total_probs\n\ndef dump_results(epoch=None):\n    train_df = pd.DataFrame({'id':train_all_tokenized['id'], 'labels':train_all_tokenized[target], \n      'probs': train_probs, 'split':['train']* len(train_all_tokenized)\n    })\n    test_df = pd.DataFrame({'id':test_all_tokenized['id'], 'labels':test_all_tokenized[target], \n      'probs': test_probs, 'split':['test']* len(test_all_tokenized)\n    })\n\n    total_df = pd.concat([train_df, test_df],ignore_index=True)\n\n    if epoch is None:\n        total_df.to_csv('results.csv', index=False)\n    else:\n        total_df.to_csv(f'results_{epoch}.csv', index=False)\n        \n    # return preds_df","metadata":{"id":"YMVfXSNXNszY","execution":{"iopub.status.busy":"2022-05-07T22:37:04.418178Z","iopub.execute_input":"2022-05-07T22:37:04.418661Z","iopub.status.idle":"2022-05-07T22:37:05.218948Z","shell.execute_reply.started":"2022-05-07T22:37:04.418612Z","shell.execute_reply":"2022-05-07T22:37:05.218227Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Data loader","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\n\ntrain_dataloader = DataLoader(train_tokenized, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_tokenized, batch_size=BATCH_SIZE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model and optimizer","metadata":{}},{"cell_type":"code","source":"model = load_pretrained_model(model_name, num_labels)\n\n# Define optimizer\nLEARNING_RATE = 1e-3\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\nEPOCHS = 3\n\n# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)","metadata":{"id":"4p90SUEiOUEU","execution":{"iopub.status.busy":"2022-05-08T01:19:24.890063Z","iopub.execute_input":"2022-05-08T01:19:24.890335Z","iopub.status.idle":"2022-05-08T01:19:27.138511Z","shell.execute_reply.started":"2022-05-08T01:19:24.890304Z","shell.execute_reply":"2022-05-08T01:19:27.137795Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Loop","metadata":{"id":"LEmY9e-XN-yR"}},{"cell_type":"code","source":"# Set the model to train mode (HuggingFace models load in eval mode)\nmodel = model.train().to(device)\n\nfor epoch in range(1, EPOCHS+1):\n    gc.collect()\n    \n    train_loss, train_result, train_probs = train(model, train_dataloader, epoch)\n    test_loss, test_result, test_probs = evaluate(model, test_dataloader, epoch)\n\n    print(\n      f\"Epoch: {epoch} | \"\n      f\"Train loss: {train_loss:.3f} | \"\n      f\"Train result: {train_result} |\\n\"\n      f\"Test loss: {test_loss:.3f} | \"\n      f\"Test result: {test_result} | \"\n    )\n    dump_results(epoch)\n    lr_scheduler.step()","metadata":{"id":"ZbJF65L_OASJ","outputId":"a6bd0251-3eac-4e8f-d8c6-381ff3fa075b","execution":{"iopub.status.busy":"2022-05-08T02:05:46.182938Z","iopub.execute_input":"2022-05-08T02:05:46.183216Z","iopub.status.idle":"2022-05-08T02:20:50.637272Z","shell.execute_reply.started":"2022-05-08T02:05:46.183185Z","shell.execute_reply":"2022-05-08T02:20:50.636524Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"|Model|Train Time|Learning rate|batch size|epoch |          |  train |         |       | test    |       |\n|-----|----------|-------------|----------|------|----------|--------|---------|-------|---------|-------|\n|     |          |             |          |      |  loss    |  f1    |  auc    | loss  | f1      | auc   |\n|bert small|10 min|     1e-5   |    128   |   1  |  0.446   | 0.7927 |  0.8698 | 0.376 |  0.382  | 0.9170|\n|     | 5min     |      1e-5   |    128   |   2  |  0.371   | 0.8351 |  0.9126 | 0.362 |  0.395  | 0.9287|\n|     | | | | | | | | | | |\n|     |  12min   |      1e-3   |    128   |   1  |    0.377 | 0.8163 |  0.9082 | 0.452 |  0.4912 | 0.9337|\n|     |  11min   |      1e-3   |    128   |   2  |    0.354 | 0.8215 |  0.9198 | 0.363 |  0.4791 | 0.9371|\n|     |  11min   |      1e-3   |    128   |   3  |    0.348 | 0.8383 |  0.9229 | 0.419 |  0.4679 | 0.9388|\n|     |  11min   |      1e-3   |    128   |   4  |    0.342 | 0.8428 |  0.9264 | 0.344 |  0.5287 | 0.9402|\n|     | | | | | | | | | | |\n|bert small with lr scheduler|11min|1e-3|128| 1  |    0.377 | 0.8261 |  0.9097 | 0.419 |  0.4013 | 0.9341|\n|     |          |    1e-3     |    128   |   2  |    0.347 | 0.8472 |  0.9250 | 0.391 |  0.4877 | 0.9394|\n|     |          |    1e-3     |    128   |   3  |    0.333 | 0.8530 |  0.9312 | 0.336 |  0.5167 | 0.9422|\n|     |          |    1e-3     |    128   |   4  |    0.320 | 0.8573 |  0.9363 | 0.305 |  0.5329 | 0.9448|\n|     | | | | | | | | | | |\n|     |          |     1e-4    |    128   |   1  |  0.358   | 0.8403 |  0.9174 | 0.335 |  0.4418 | 0.9434|\n|     |          |      1e-4   |    128   |   2  |  0.312   | 0.8654 |  0.9381 | 0.305 |  0.4645 | 0.9486|\n|     |          |      1e-4   |    128   |   3  |  0.297   | 0.8720 |  0.9438 | 0.300 |  0.4666 | 0.9503|\n|     |          |      1e-4   |    128   |   4  |  0.286   | 0.8771 |  0.9477 | 0.304 |  0.4628 | 0.9506|\n","metadata":{}}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import sys\n",
    "# this adds the src folder in the sys path, where the metric_utils.py file is\n",
    "# not needed if this notebook is in the same folder, but uncomment to access from the data subfolders\n",
    "sys.path.append( '..' )\n",
    "from metric_utils import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ucberkeley hate speech'\n",
    "model_name = 'bert'\n",
    "\n",
    "# change to ../../results/{dataset_name} when using inside one of the data subfolders\n",
    "result_folder = f'../../results/{dataset_name}'\n",
    "test_csv_filepath = os.path.join(result_folder, 'test.csv')\n",
    "\n",
    "model_folder = os.path.join(result_folder, model_name) # for this particular model\n",
    "normal_folder = os.path.join(model_folder, 'normal')\n",
    "dp_folder = os.path.join(model_folder, 'epsilon 3.0')\n",
    "\n",
    "result_filepath = os.path.join(normal_folder, 'results.csv')\n",
    "dp_result_filepath = os.path.join(dp_folder, 'results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset can be preprocessed from the original dataset to be used here. To simplify things, I saved the preprocessed datasets during the tokenizing process as csv files and then downloaded them in the corresponding dataset folder of [`result`](../results/) directory.\n",
    "\n",
    "You can recreate the processed datasets using the tokenize notebooks for that particular dataset. That would give you train, test and validation csv files as well as the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(result_filepath)\n",
    "dp_result = pd.read_csv(dp_result_filepath)\n",
    "df = pd.read_csv(test_csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>target_gender_men</th>\n",
       "      <th>target_gender_women</th>\n",
       "      <th>target_race_white</th>\n",
       "      <th>target_race_black</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41012</td>\n",
       "      <td>I don't want to see America treated like a garbage can. Luv from Canada.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31056</td>\n",
       "      <td>@mike_pence @realDonaldTrump You're a Christian of convenience. You're a fake</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  \\\n",
       "0       41012   \n",
       "1       31056   \n",
       "\n",
       "                                                                            text  \\\n",
       "0       I don't want to see America treated like a garbage can. Luv from Canada.   \n",
       "1  @mike_pence @realDonaldTrump You're a Christian of convenience. You're a fake   \n",
       "\n",
       "   target_gender_men  target_gender_women  target_race_white  \\\n",
       "0                  0                    0                  0   \n",
       "1                  0                    0                  0   \n",
       "\n",
       "   target_race_black  labels  \n",
       "0                  0       0  \n",
       "1                  0       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only calculate test result\n",
    "result = result[result['split']=='test']\n",
    "dp_result = dp_result[dp_result['split']=='test']\n",
    "\n",
    "# drop split column\n",
    "result.drop(columns=['split'], inplace=True)\n",
    "dp_result.drop(columns=['split'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result has id column which is the same as the text ids from raw dataset\n",
    "raw_id_column = 'comment_id'\n",
    "df.rename({raw_id_column: id_column}, axis=1, inplace=True)\n",
    "df.drop(columns=['labels'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.merge(df, on=id_column, how='inner').reset_index(drop=True)\n",
    "dp_result = dp_result.merge(df, on=id_column, how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>probs</th>\n",
       "      <th>text</th>\n",
       "      <th>target_gender_men</th>\n",
       "      <th>target_gender_women</th>\n",
       "      <th>target_race_white</th>\n",
       "      <th>target_race_black</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41012</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502331</td>\n",
       "      <td>I don't want to see America treated like a garbage can. Luv from Canada.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31056</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214350</td>\n",
       "      <td>@mike_pence @realDonaldTrump You're a Christian of convenience. You're a fake</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17058</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370149</td>\n",
       "      <td>Jesus just has his holy reverse card that he uses on the microbes. Does that girl have a holy reverse? I thot not.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083863</td>\n",
       "      <td>Why? It's never even hinted that being gay or bi means you end up in the Bad Place, if that's what you meant?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3792</td>\n",
       "      <td>0</td>\n",
       "      <td>0.673117</td>\n",
       "      <td>I'm half Jewish you fucking idiot. Eat a dick</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  labels     probs  \\\n",
       "0  41012       0  0.502331   \n",
       "1  31056       0  0.214350   \n",
       "2  17058       0  0.370149   \n",
       "3   4023       0  0.083863   \n",
       "4   3792       0  0.673117   \n",
       "\n",
       "                                                                                                                 text  \\\n",
       "0                                            I don't want to see America treated like a garbage can. Luv from Canada.   \n",
       "1                                       @mike_pence @realDonaldTrump You're a Christian of convenience. You're a fake   \n",
       "2  Jesus just has his holy reverse card that he uses on the microbes. Does that girl have a holy reverse? I thot not.   \n",
       "3       Why? It's never even hinted that being gay or bi means you end up in the Bad Place, if that's what you meant?   \n",
       "4                                                                       I'm half Jewish you fucking idiot. Eat a dick   \n",
       "\n",
       "   target_gender_men  target_gender_women  target_race_white  \\\n",
       "0                  0                    0                  0   \n",
       "1                  0                    0                  0   \n",
       "2                  0                    0                  0   \n",
       "3                  0                    0                  0   \n",
       "4                  0                    0                  0   \n",
       "\n",
       "   target_race_black  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert probability to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[prediction_column] = result[probability_column] >=0.5\n",
    "dp_result[prediction_column] = dp_result[probability_column] >=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['target_gender_women', 'target_gender_men', 'target_race_black', 'target_race_white']\n"
     ]
    }
   ],
   "source": [
    "group_map = {\n",
    "    'gender': {\n",
    "        'unprivileged':['target_gender_women'],\n",
    "        'privileged':['target_gender_men']\n",
    "    },\n",
    "    'race': {\n",
    "        'unprivileged':['target_race_black'],\n",
    "        'privileged': ['target_race_white']\n",
    "    }\n",
    "}\n",
    "\n",
    "identities = []\n",
    "for group_key in group_map.keys():\n",
    "    subgroup_map = group_map[group_key]\n",
    "    for subgroup_key in subgroup_map.keys():\n",
    "        identities.extend(subgroup_map[subgroup_key])\n",
    "\n",
    "print(identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize identity and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = binarize(result, [target_column] + identities)\n",
    "dp_result = binarize(dp_result, [target_column] + identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fairness_metrics</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender_DP</th>\n",
       "      <th>race</th>\n",
       "      <th>race_DP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demographic parity</td>\n",
       "      <td>0.468336</td>\n",
       "      <td>0.611193</td>\n",
       "      <td>0.517110</td>\n",
       "      <td>0.543726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Equality of Opportunity (w.r.t y = 1)</td>\n",
       "      <td>0.923097</td>\n",
       "      <td>0.979236</td>\n",
       "      <td>0.699580</td>\n",
       "      <td>0.633739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Equality of Opportunity (w.r.t y = 0)</td>\n",
       "      <td>0.933718</td>\n",
       "      <td>0.971290</td>\n",
       "      <td>0.922979</td>\n",
       "      <td>0.927503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Equality of Odds</td>\n",
       "      <td>0.928407</td>\n",
       "      <td>0.975263</td>\n",
       "      <td>0.811279</td>\n",
       "      <td>0.780621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unprotected-accuracy</td>\n",
       "      <td>0.756967</td>\n",
       "      <td>0.734284</td>\n",
       "      <td>0.802680</td>\n",
       "      <td>0.780755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>protected-accuracy</td>\n",
       "      <td>0.796760</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>0.787072</td>\n",
       "      <td>0.764259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.776863</td>\n",
       "      <td>0.753740</td>\n",
       "      <td>0.794876</td>\n",
       "      <td>0.772507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        fairness_metrics    gender  gender_DP      race  \\\n",
       "0                     demographic parity  0.468336   0.611193  0.517110   \n",
       "1  Equality of Opportunity (w.r.t y = 1)  0.923097   0.979236  0.699580   \n",
       "2  Equality of Opportunity (w.r.t y = 0)  0.933718   0.971290  0.922979   \n",
       "3                       Equality of Odds  0.928407   0.975263  0.811279   \n",
       "4                   unprotected-accuracy  0.756967   0.734284  0.802680   \n",
       "5                     protected-accuracy  0.796760   0.773196  0.787072   \n",
       "6                               accuracy  0.776863   0.753740  0.794876   \n",
       "\n",
       "    race_DP  \n",
       "0  0.543726  \n",
       "1  0.633739  \n",
       "2  0.927503  \n",
       "3  0.780621  \n",
       "4  0.780755  \n",
       "5  0.764259  \n",
       "6  0.772507  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_results = {\n",
    "   'fairness_metrics': ['demographic parity', 'Equality of Opportunity (w.r.t y = 1)',\n",
    "   'Equality of Opportunity (w.r.t y = 0)', 'Equality of Odds', 'unprotected-accuracy',\n",
    "   'protected-accuracy', 'accuracy']\n",
    "}\n",
    "\n",
    "for group_key in group_map.keys():\n",
    "   subgroup_map = group_map[group_key]\n",
    "   privileged_group = subgroup_map['privileged']\n",
    "   unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "   bias_results[group_key] = calculate_bias(result, privileged_group, unprivileged_group)\n",
    "   bias_results[group_key+'_DP'] = calculate_bias(dp_result, privileged_group, unprivileged_group)\n",
    "\n",
    "bias_results = pd.DataFrame(bias_results) \n",
    "bias_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_results.to_csv(os.path.join(dp_folder, 'bias.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>target_gender_men</th>\n",
       "      <th>target_gender_men_DP</th>\n",
       "      <th>target_gender_women</th>\n",
       "      <th>target_gender_women_DP</th>\n",
       "      <th>target_race_white</th>\n",
       "      <th>target_race_white_DP</th>\n",
       "      <th>target_race_black</th>\n",
       "      <th>target_race_black_DP</th>\n",
       "      <th>Total</th>\n",
       "      <th>Total_DP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.823891</td>\n",
       "      <td>0.790713</td>\n",
       "      <td>0.816284</td>\n",
       "      <td>0.782050</td>\n",
       "      <td>0.788782</td>\n",
       "      <td>0.737971</td>\n",
       "      <td>0.877634</td>\n",
       "      <td>0.858890</td>\n",
       "      <td>0.858167</td>\n",
       "      <td>0.821560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.796760</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>0.756967</td>\n",
       "      <td>0.734284</td>\n",
       "      <td>0.787072</td>\n",
       "      <td>0.764259</td>\n",
       "      <td>0.802680</td>\n",
       "      <td>0.780755</td>\n",
       "      <td>0.806742</td>\n",
       "      <td>0.789628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.594118</td>\n",
       "      <td>0.524691</td>\n",
       "      <td>0.635569</td>\n",
       "      <td>0.552402</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.772472</td>\n",
       "      <td>0.730539</td>\n",
       "      <td>0.637484</td>\n",
       "      <td>0.562073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.619632</td>\n",
       "      <td>0.578231</td>\n",
       "      <td>0.624046</td>\n",
       "      <td>0.615572</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.759669</td>\n",
       "      <td>0.767296</td>\n",
       "      <td>0.665088</td>\n",
       "      <td>0.665814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.570621</td>\n",
       "      <td>0.480226</td>\n",
       "      <td>0.647525</td>\n",
       "      <td>0.500990</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.330882</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.697143</td>\n",
       "      <td>0.612080</td>\n",
       "      <td>0.486301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>false positive rate</td>\n",
       "      <td>0.123506</td>\n",
       "      <td>0.123506</td>\n",
       "      <td>0.189788</td>\n",
       "      <td>0.152216</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.084615</td>\n",
       "      <td>0.184713</td>\n",
       "      <td>0.157113</td>\n",
       "      <td>0.118449</td>\n",
       "      <td>0.093802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               metrics  target_gender_men  target_gender_men_DP  \\\n",
       "0                  auc           0.823891              0.790713   \n",
       "1             accuracy           0.796760              0.773196   \n",
       "2             f1_score           0.594118              0.524691   \n",
       "3            precision           0.619632              0.578231   \n",
       "4               recall           0.570621              0.480226   \n",
       "5  false positive rate           0.123506              0.123506   \n",
       "\n",
       "   target_gender_women  target_gender_women_DP  target_race_white  \\\n",
       "0             0.816284                0.782050           0.788782   \n",
       "1             0.756967                0.734284           0.787072   \n",
       "2             0.635569                0.552402           0.540984   \n",
       "3             0.624046                0.615572           0.611111   \n",
       "4             0.647525                0.500990           0.485294   \n",
       "5             0.189788                0.152216           0.107692   \n",
       "\n",
       "   target_race_white_DP  target_race_black  target_race_black_DP     Total  \\\n",
       "0              0.737971           0.877634              0.858890  0.858167   \n",
       "1              0.764259           0.802680              0.780755  0.806742   \n",
       "2              0.420561           0.772472              0.730539  0.637484   \n",
       "3              0.576923           0.759669              0.767296  0.665088   \n",
       "4              0.330882           0.785714              0.697143  0.612080   \n",
       "5              0.084615           0.184713              0.157113  0.118449   \n",
       "\n",
       "   Total_DP  \n",
       "0  0.821560  \n",
       "1  0.789628  \n",
       "2  0.562073  \n",
       "3  0.665814  \n",
       "4  0.486301  \n",
       "5  0.093802  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results = {\n",
    "    'metrics': ['auc', 'accuracy', 'f1_score', 'precision', 'recall', 'false positive rate']\n",
    "}\n",
    "\n",
    "for group_key in group_map.keys():\n",
    "    subgroup_map = group_map[group_key]\n",
    "    privileged_group = subgroup_map['privileged']\n",
    "    unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "    privileged_group_name = ','.join(privileged_group)\n",
    "    unprivileged_group_name = ','.join(unprivileged_group)\n",
    "\n",
    "    overall_results[privileged_group_name] = calculate_metrics(result, privileged_group)\n",
    "    overall_results[privileged_group_name + '_DP'] = calculate_metrics(dp_result, privileged_group)\n",
    "\n",
    "    overall_results[unprivileged_group_name] = calculate_metrics(result, unprivileged_group)\n",
    "    overall_results[unprivileged_group_name + '_DP'] = calculate_metrics(dp_result, unprivileged_group)\n",
    "\n",
    "overall_results['Total'] = calculate_metrics(result, [])\n",
    "overall_results['Total_DP'] = calculate_metrics(dp_result, [])\n",
    "\n",
    "overall_results = pd.DataFrame(overall_results) \n",
    "overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results.to_csv(os.path.join(dp_folder, 'overall_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch benchmarking\n",
    "Benchmark all models and privacy budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['bert', 'distilbert']:\n",
    "    model_folder = os.path.join(result_folder, model_name) # for this particular model\n",
    "    normal_folder = os.path.join(model_folder, 'normal')\n",
    "    result_filepath = os.path.join(normal_folder, 'results.csv')\n",
    "\n",
    "    result = pd.read_csv(result_filepath)\n",
    "    result = result[result['split']=='test']\n",
    "    # drop split column\n",
    "    result.drop(columns=['split'], inplace=True)\n",
    "\n",
    "    result = result.merge(df, on=id_column, how='inner').reset_index(drop=True)\n",
    "    result[prediction_column] = result[probability_column] >=0.5\n",
    "    result = binarize(result, [target_column] + identities)\n",
    "\n",
    "    \n",
    "    for epsilon in [3.0, 6.0, 9.0]:\n",
    "        dp_folder = os.path.join(model_folder, f'epsilon {epsilon}')\n",
    "        dp_result_filepath = os.path.join(dp_folder, 'results.csv')\n",
    "        dp_result = pd.read_csv(dp_result_filepath)\n",
    "\n",
    "        # only calculate test result\n",
    "        dp_result = dp_result[dp_result['split']=='test']\n",
    "        dp_result.drop(columns=['split'], inplace=True)\n",
    "        dp_result = dp_result.merge(df, on=id_column, how='inner').reset_index(drop=True)\n",
    "        \n",
    "        dp_result[prediction_column] = dp_result[probability_column] >=0.5\n",
    "        dp_result = binarize(dp_result, [target_column] + identities)\n",
    "\n",
    "        bias_results = {\n",
    "        'fairness_metrics': ['demographic parity', 'Equality of Opportunity (w.r.t y = 1)',\n",
    "        'Equality of Opportunity (w.r.t y = 0)', 'Equality of Odds', 'unprotected-accuracy',\n",
    "        'protected-accuracy', 'accuracy']\n",
    "        }\n",
    "\n",
    "        for group_key in group_map.keys():\n",
    "            subgroup_map = group_map[group_key]\n",
    "            privileged_group = subgroup_map['privileged']\n",
    "            unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "            bias_results[group_key] = calculate_bias(result, privileged_group, unprivileged_group)\n",
    "            bias_results[group_key+'_DP'] = calculate_bias(dp_result, privileged_group, unprivileged_group)\n",
    "\n",
    "        bias_results = pd.DataFrame(bias_results) \n",
    "        bias_results.round(3).to_csv(os.path.join(dp_folder, 'bias.csv'), index=False)\n",
    "\n",
    "\n",
    "        overall_results = {\n",
    "            'metrics': ['auc', 'accuracy', 'f1_score', 'precision', 'recall', 'false positive rate']\n",
    "        }\n",
    "\n",
    "        for group_key in group_map.keys():\n",
    "            subgroup_map = group_map[group_key]\n",
    "            privileged_group = subgroup_map['privileged']\n",
    "            unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "            privileged_group_name = ','.join(privileged_group)\n",
    "            unprivileged_group_name = ','.join(unprivileged_group)\n",
    "\n",
    "            overall_results[privileged_group_name] = calculate_metrics(result, privileged_group)\n",
    "            overall_results[privileged_group_name + '_DP'] = calculate_metrics(dp_result, privileged_group)\n",
    "\n",
    "            overall_results[unprivileged_group_name] = calculate_metrics(result, unprivileged_group)\n",
    "            overall_results[unprivileged_group_name + '_DP'] = calculate_metrics(dp_result, unprivileged_group)\n",
    "\n",
    "        overall_results['Total'] = calculate_metrics(result, [])\n",
    "        overall_results['Total_DP'] = calculate_metrics(dp_result, [])\n",
    "\n",
    "        overall_results = pd.DataFrame(overall_results) \n",
    "        overall_results.columns = [col.replace('target_', '') for col in overall_results.columns]\n",
    "        overall_results.round(3).to_csv(os.path.join(dp_folder, 'overall_results.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

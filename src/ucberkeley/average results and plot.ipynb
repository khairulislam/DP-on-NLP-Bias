{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that they have 'target_gender' or 'target_race' prefix, but we'll replace them later\n",
    "identities = ['target_gender_men', 'target_gender_women','target_gender_transgender', 'target_race_white', 'target_race_black', 'target_race_asian', ]\n",
    "categories = ['men vs women', 'men vs transgender', 'white vs black', 'white vs asian']\n",
    "figsize = (14,8)\n",
    "epsilon_list = [0.5, 1.0, 3.0, 6.0, 9.0]\n",
    "dataset_name = 'ucberkeley'\n",
    "dataset_directory = f'../../results/{dataset_name}/'\n",
    "model_name = 'bert-base-uncased'\n",
    "output_folder = f'{dataset_directory}/average/{model_name}'\n",
    "runs = 3\n",
    "linewidth = 3.5\n",
    "markersize = 13\n",
    "markers = ['o', 'v', 's', 'x',  'h', 'D', '^', '>', 'p', '<', '*', 'P',  '.', '+']\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average bias and overall results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load bias and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_df_dict = {}\n",
    "overall_results_dict = {}\n",
    "\n",
    "for run in range(1, runs+1):\n",
    "    run_folder = f'{dataset_directory}/run {run}'\n",
    "    model_folder = os.path.join(run_folder, model_name)\n",
    "\n",
    "    normal_folder = os.path.join(model_folder, 'normal')\n",
    "    bias_df = pd.read_csv(os.path.join(normal_folder, 'bias.csv'))\n",
    "    overall_results = pd.read_csv(os.path.join(normal_folder, 'overall_results.csv'))\n",
    "    if run ==1:\n",
    "        bias_df_dict['None'] = [bias_df]\n",
    "        overall_results_dict['None'] = [overall_results]\n",
    "    else:\n",
    "        bias_df_dict['None'].append(bias_df)\n",
    "        overall_results_dict['None'].append(overall_results)\n",
    "\n",
    "    for epsilon in epsilon_list:\n",
    "        dp_folder = os.path.join(model_folder, f'epsilon {epsilon}')\n",
    "        \n",
    "        bias_df = pd.read_csv(os.path.join(dp_folder, 'bias.csv'))\n",
    "        overall_results = pd.read_csv(os.path.join(dp_folder, 'overall_results.csv'))\n",
    "\n",
    "        if run ==1:\n",
    "            bias_df_dict[epsilon] = [bias_df]\n",
    "            overall_results_dict[epsilon] = [overall_results]\n",
    "        else:\n",
    "            bias_df_dict[epsilon].append(bias_df)\n",
    "            overall_results_dict[epsilon].append(overall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate. Dump average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in ['None'] + epsilon_list:\n",
    "    bias_df_dict[epsilon] = pd.concat(bias_df_dict[epsilon])\n",
    "    # mean = bias_df_dict[epsilon].groupby('fairness_metrics').aggregate('mean').reset_index()\n",
    "    # mean.round(3).to_csv(\n",
    "    #     os.path.join(output_folder, f'bias_epsilon_{epsilon}.csv'), \n",
    "    #     index=False\n",
    "    # )\n",
    "\n",
    "    overall_results_dict[epsilon] = pd.concat(overall_results_dict[epsilon])\n",
    "    # mean = overall_results_dict[epsilon].groupby('metrics').aggregate('mean').reset_index()\n",
    "    # mean.round(3).to_csv(\n",
    "    #     os.path.join(output_folder, f'overall_results_epsilon_{epsilon}.csv'), \n",
    "    #     index=False\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "# Apply the default theme\n",
    "sns.set_theme()\n",
    "sns.set(font_scale = 2)\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.markersize'] = markersize\n",
    "mpl.rcParams['lines.linewidth'] = linewidth\n",
    "\n",
    "formatter = StrMethodFormatter('{x:,.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'Subgroup': [],\n",
    "    'Metric': [],\n",
    "    'Train Type': []\n",
    "}\n",
    "metric= 'EqOdd'\n",
    "\n",
    "def key_map(key):\n",
    "    if key=='None':\n",
    "        return 'Non-DP'\n",
    "    return f'Îµ <={key}'\n",
    "\n",
    "for category in categories:\n",
    "    for key in bias_df_dict.keys():\n",
    "        bias_df = bias_df_dict[key]\n",
    "        values = bias_df[bias_df['fairness_metrics']==metric][category].values\n",
    "        dictionary['Metric'].extend(values)\n",
    "        dictionary['Subgroup'].extend([category]*len(values))\n",
    "        dictionary['Train Type'].extend([key_map(key)]*len(values))\n",
    "\n",
    "df = pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = df['Train Type'].unique()\n",
    "hue_order = [hue_order[0]] + sorted(hue_order[1:], reverse=True)\n",
    "hue_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this returns the df with an increasing privacy order of Train type\n",
    "def order(df):\n",
    "    return df.set_index('Train Type').reindex(hue_order).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figsize)\n",
    "sns.boxplot(x = 'Subgroup', y = 'Metric', hue='Train Type', hue_order=hue_order, data = df, linewidth=2.5)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(os.path.join(output_folder, 'eodds_boxplot.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for epsilon in ['None']+epsilon_list:\n",
    "    mean = overall_results_dict[epsilon].groupby('metrics').aggregate('mean').reset_index()\n",
    "    mean['Train Type'] = key_map(epsilon)\n",
    "    total.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat(total)\n",
    "total_df.rename({col:'_'.join(col.split('_')[2:]) for col in total_df.columns if 'target_' in col}, axis=1, inplace=True)\n",
    "identities = ['_'.join(identity.split('_')[2:]) for identity in identities]\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df[total_df['metrics'].isin(['accuracy', 'f1_score', 'auc'])][['metrics', 'Total', 'Train Type']].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metrics = ['auc', 'f1_score', 'accuracy', 'precision', 'recall']\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "for i, selected_metric in enumerate(selected_metrics):\n",
    "    selected_df = total_df[total_df['metrics']==selected_metric].round(3)\n",
    "    selected_df = order(selected_df)\n",
    "    sns.lineplot(x=selected_df[\"Train Type\"], y=selected_df[\"Total\"], label=selected_metric, marker=markers[i])\n",
    "\n",
    "plt.legend(loc='upper right', fancybox=True, framealpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "plt.savefig(os.path.join(output_folder, 'overall_total.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util plot_overall_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overall_metric(selected_metric, title=None, figure_name=None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    selected_df = total_df[total_df['metrics']==selected_metric]\n",
    "    selected_df = order(selected_df)\n",
    "\n",
    "    for i, identity in enumerate(identities):\n",
    "        plt.plot(selected_df[\"Train Type\"], selected_df[identity], label=identity, marker=markers[i])\n",
    "\n",
    "    plt.legend(loc='upper right', fancybox=True, framealpha=0.4)\n",
    "\n",
    "    # https://stackoverflow.com/questions/29188757/matplotlib-specify-format-of-floats-for-tick-labels\n",
    "    plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if figure_name:\n",
    "        plt.savefig(os.path.join(output_folder, figure_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('bnsp_auc', figure_name='bnsp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('bpsn_auc', figure_name= 'bpsn.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('auc', figure_name='roc_auc.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('f1_score', figure_name= 'f1_score.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('false positive rate', figure_name= 'false_positive_rate.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('precision', figure_name= 'precision.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('recall', figure_name= 'recall.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for epsilon in ['None']+epsilon_list:\n",
    "    mean = bias_df_dict[epsilon].groupby('fairness_metrics').aggregate('mean').reset_index()\n",
    "    mean['Train Type'] = key_map(epsilon)\n",
    "    total.append(mean)\n",
    "\n",
    "total_df = pd.concat(total)\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util plot_bias_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_metric(category, title=None, figure_name= None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    metric_column = 'fairness_metrics'\n",
    "    # selected_metrics = total_df[metric_column].unique()\n",
    "    selected_metrics = ['EqOdd','EqOpp1', 'EqOpp0',  'parity', 'p-accuracy', 'up-accuracy']\n",
    "    \n",
    "    for i, selected_metric in enumerate(selected_metrics):\n",
    "        selected_df = total_df[total_df[metric_column]==selected_metric]\n",
    "        selected_df = order(selected_df)\n",
    "        \n",
    "        plt.plot(selected_df[\"Train Type\"], selected_df[category], label=selected_metric, marker=markers[i])\n",
    "\n",
    "    plt.legend(loc='upper right', fancybox=True, framealpha=0.5)\n",
    "\n",
    "    plt.gca().yaxis.set_major_formatter(formatter)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if figure_name:\n",
    "        plt.savefig(os.path.join(output_folder, figure_name))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in ['white vs black', 'white vs asian']:\n",
    "    plot_bias_metric(category, figure_name=f\"racial_bias_{'_'.join(category.split())}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in ['men vs women', 'men vs transgender']:\n",
    "    plot_bias_metric(category, figure_name=f\"gender_bias_{'_'.join(category.split())}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that they have 'target_gender' or 'target_race' prefix, but we'll replace them later\n",
    "identities = ['male','female', 'white','black'] \n",
    "categories = ['gender', 'race']\n",
    "figsize = (14,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average bias and overall results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load bias and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'jigsaw'\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "bias_df_dict = {}\n",
    "overall_results_dict = {}\n",
    "\n",
    "for run in range(1, 4):\n",
    "    run_folder = f'../../results/{dataset_name}/run {run}'\n",
    "    model_folder = os.path.join(run_folder, model_name)\n",
    "\n",
    "    normal_folder = os.path.join(model_folder, 'normal')\n",
    "    bias_df = pd.read_csv(os.path.join(normal_folder, 'bias.csv'))\n",
    "    overall_results = pd.read_csv(os.path.join(normal_folder, 'overall_results.csv'))\n",
    "    if run ==1:\n",
    "        bias_df_dict['None'] = [bias_df]\n",
    "        overall_results_dict['None'] = [overall_results]\n",
    "    else:\n",
    "        bias_df_dict['None'].append(bias_df)\n",
    "        overall_results_dict['None'].append(overall_results)\n",
    "\n",
    "    for epsilon in [3.0, 6.0, 9.0]:\n",
    "        dp_folder = os.path.join(model_folder, f'epsilon {epsilon}')\n",
    "        \n",
    "        bias_df = pd.read_csv(os.path.join(dp_folder, 'bias.csv'))\n",
    "        overall_results = pd.read_csv(os.path.join(dp_folder, 'overall_results.csv'))\n",
    "\n",
    "        if run ==1:\n",
    "            bias_df_dict[epsilon] = [bias_df]\n",
    "            overall_results_dict[epsilon] = [overall_results]\n",
    "        else:\n",
    "            bias_df_dict[epsilon].append(bias_df)\n",
    "            overall_results_dict[epsilon].append(overall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate. Dump average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = f'../../results/{dataset_name}/average/{model_name}'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for epsilon in ['None', 3.0, 6.0, 9.0]:\n",
    "    bias_df_dict[epsilon] = pd.concat(bias_df_dict[epsilon])\n",
    "    mean = bias_df_dict[epsilon].groupby('fairness_metrics').aggregate('mean').reset_index()\n",
    "    mean.round(3).to_csv(\n",
    "        os.path.join(output_folder, f'bias_epsilon_{epsilon}_mean.csv'), \n",
    "        index=False\n",
    "    )\n",
    "    std = bias_df_dict[epsilon].groupby('fairness_metrics').aggregate('std').reset_index()\n",
    "    std.round(3).to_csv(\n",
    "        os.path.join(output_folder, f'bias_epsilon_{epsilon}_std.csv'), \n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    overall_results_dict[epsilon] = pd.concat(overall_results_dict[epsilon])\n",
    "    mean = overall_results_dict[epsilon].groupby('metrics').aggregate('mean').reset_index()\n",
    "    mean.round(3).to_csv(\n",
    "        os.path.join(output_folder, f'overall_results_epsilon_{epsilon}_mean.csv'), \n",
    "        index=False\n",
    "    )\n",
    "    std = overall_results_dict[epsilon].groupby('metrics').aggregate('std').reset_index()\n",
    "    std.round(3).to_csv(\n",
    "        os.path.join(output_folder, f'overall_results_epsilon_{epsilon}_std.csv'), \n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Apply the default theme\n",
    "sns.set_theme()\n",
    "sns.set(font_scale = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    'Subgroup': [],\n",
    "    'Metric': [],\n",
    "    'Train Type': []\n",
    "}\n",
    "metric= 'EqOdd'\n",
    "\n",
    "def key_map(key):\n",
    "    if key=='None':\n",
    "        return 'Non-private'\n",
    "    return f'Îµ <={key}'\n",
    "\n",
    "for category in categories:\n",
    "    for key in bias_df_dict.keys():\n",
    "        bias_df = bias_df_dict[key]\n",
    "        values = bias_df[bias_df['fairness_metrics']==metric][category].values\n",
    "        dictionary['Metric'].extend(values)\n",
    "        dictionary['Subgroup'].extend([category]*len(values))\n",
    "        dictionary['Train Type'].extend([key_map(key)]*len(values))\n",
    "\n",
    "df = pd.DataFrame(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=figsize)\n",
    "sns.boxplot(x = 'Subgroup', y = 'Metric', hue='Train Type', data = df)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, 'eodds_boxplot.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped = df.groupby(['Train Type', 'Subgroup'])[['Metric']].agg('mean').reset_index()\n",
    "# plt.figure(figsize=(16,8))\n",
    "# sns.lineplot(x ='Train Type', y = 'Metric', hue='Subgroup', data = grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for epsilon in ['None', 9.0, 6.0, 3.0]:\n",
    "    mean = overall_results_dict[epsilon].groupby('metrics').aggregate('mean').reset_index()\n",
    "    mean['Train Type'] = key_map(epsilon)\n",
    "    total.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat(total)\n",
    "total_df.rename({col:col.split('_')[-1] for col in total_df.columns if 'target_' in col}, axis=1, inplace=True)\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metrics = ['auc', 'f1_score', 'accuracy', 'false positive rate', 'precision', 'recall']\n",
    "plt.figure(figsize=figsize)\n",
    "for selected_metric in selected_metrics:\n",
    "    selected_df = total_df[total_df['metrics']==selected_metric]\n",
    "    plt.plot(selected_df[\"Train Type\"], selected_df[\"Total\"], label=selected_metric)\n",
    "\n",
    "plt.legend(loc='upper right', fancybox=True, framealpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_folder, 'overall_total.pdf'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util plot_overall_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overall_metric(selected_metric, title=None, figure_name=None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    selected_df = total_df[total_df['metrics']==selected_metric]\n",
    "\n",
    "    for identity in identities:\n",
    "        plt.plot(selected_df[\"Train Type\"], selected_df[identity], label=identity)\n",
    "\n",
    "    plt.legend(loc='upper right', fancybox=True, framealpha=0.4)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if figure_name:\n",
    "        plt.savefig(os.path.join(output_folder, figure_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('bnsp_auc', figure_name='bnsp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('bpsn_auc', figure_name= 'bpsn.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('auc', figure_name='roc_auc.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('f1_score', figure_name= 'f1_score.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_metric('false positive rate', figure_name= 'false_positive_rate.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "for epsilon in ['None', 9.0, 6.0, 3.0]:\n",
    "    mean = bias_df_dict[epsilon].groupby('fairness_metrics').aggregate('mean').reset_index()\n",
    "    mean['Train Type'] = key_map(epsilon)\n",
    "    total.append(mean)\n",
    "\n",
    "total_df = pd.concat(total)\n",
    "total_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util plot_bias_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_metric(category, title=None, figure_name= None):\n",
    "    plt.figure(figsize=figsize)\n",
    "    metric_column = 'fairness_metrics'\n",
    "\n",
    "    selected_metrics = total_df[metric_column].unique()\n",
    "\n",
    "    for selected_metric in selected_metrics:\n",
    "        selected_df = total_df[total_df[metric_column]==selected_metric]\n",
    "        plt.plot(selected_df[\"Train Type\"], selected_df[category], label=selected_metric)\n",
    "\n",
    "    plt.legend(loc='upper right', fancybox=True, framealpha=0.5)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if figure_name:\n",
    "        plt.savefig(os.path.join(output_folder, figure_name))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_metric('race', figure_name='racial_bias.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_metric('gender', figure_name='gender_bias.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

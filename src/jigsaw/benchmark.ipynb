{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import sys\n",
    "# this adds the src folder in the sys path, where the metric_utils.py file is\n",
    "# not needed if this notebook is in the same folder, but uncomment to access from the data subfolders\n",
    "sys.path.append( '..' )\n",
    "from metric_utils import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'jigsaw'\n",
    "model_name = 'bert'\n",
    "run_name = \"run 1\"\n",
    "\n",
    "# change to f'../../results/{dataset_name}' when using inside one of the data subfolders\n",
    "run_folder = f'../../results/{dataset_name}/{run_name}'\n",
    "test_csv_filepath = os.path.join(run_folder, 'test.csv')\n",
    "\n",
    "model_folder = os.path.join(run_folder, model_name) # for this particular model\n",
    "normal_folder = os.path.join(model_folder, 'normal')\n",
    "dp_folder = os.path.join(model_folder, 'epsilon 3.0')\n",
    "\n",
    "result_filepath = os.path.join(normal_folder, 'results.csv')\n",
    "dp_result_filepath = os.path.join(dp_folder, 'results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset can be preprocessed from the original dataset to be used here. To simplify things, I saved the preprocessed datasets during the tokenizing process as csv files and then downloaded them in the corresponding dataset folder of [`result`](../results/) directory.\n",
    "\n",
    "You can recreate the processed datasets using the tokenize notebooks for that particular dataset. That would give you train, test and validation csv files as well as the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "      <th>asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7084460</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7141509</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  labels  male  female  transgender  white  black  asian\n",
       "0  7084460       1   NaN     NaN          NaN    NaN    NaN    NaN\n",
       "1  7141509       1   NaN     NaN          NaN    NaN    NaN    NaN"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv(result_filepath)\n",
    "dp_result = pd.read_csv(dp_result_filepath)\n",
    "\n",
    "test_df = pd.read_csv(test_csv_filepath)\n",
    "# df.drop(columns=['comment_text', 'labels'], inplace=True)\n",
    "# df.to_csv(test_csv_filepath, index=False)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing this reduces the test file size, makes things easier with git\n",
    "if 'comment_text' in test_df.columns:\n",
    "    test_df.drop(columns='comment_text', inplace=True)\n",
    "    test_df.to_csv(test_csv_filepath, index=False)\n",
    "\n",
    "test_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only calculate test result\n",
    "result = result[result['split']=='test']\n",
    "dp_result = dp_result[dp_result['split']=='test']\n",
    "\n",
    "# drop split column\n",
    "result.drop(columns=['split'], inplace=True)\n",
    "dp_result.drop(columns=['split'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_columns = [col for col in df.columns if col in result.columns and col!=id_column]\n",
    "\n",
    "result = result.merge(df.drop(columns=extra_columns), on=id_column, how='inner').reset_index(drop=True)\n",
    "dp_result = dp_result.merge(df.drop(columns=extra_columns), on=id_column, how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert probability to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[prediction_column] = result[probability_column] >=0.5\n",
    "dp_result[prediction_column] = dp_result[probability_column] >=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female', 'male', 'black', 'white']\n"
     ]
    }
   ],
   "source": [
    "group_map = {\n",
    "    'gender': {\n",
    "        'unprivileged':['female'],\n",
    "        'privileged':['male']\n",
    "    },\n",
    "    'race': {\n",
    "        'unprivileged':['black'],\n",
    "        'privileged': ['white']\n",
    "    }\n",
    "}\n",
    "\n",
    "identities = []\n",
    "for group_key in group_map.keys():\n",
    "    subgroup_map = group_map[group_key]\n",
    "    for subgroup_key in subgroup_map.keys():\n",
    "        identities.extend(subgroup_map[subgroup_key])\n",
    "\n",
    "print(identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize identity and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = binarize(result, [target_column] + identities)\n",
    "dp_result = binarize(dp_result, [target_column] + identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fairness_metrics</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender_DP</th>\n",
       "      <th>race</th>\n",
       "      <th>race_DP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demographic parity</td>\n",
       "      <td>0.998580</td>\n",
       "      <td>0.975852</td>\n",
       "      <td>0.898132</td>\n",
       "      <td>0.797963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Equality of Opportunity (w.r.t y = 1)</td>\n",
       "      <td>0.946161</td>\n",
       "      <td>0.978125</td>\n",
       "      <td>0.916765</td>\n",
       "      <td>0.903395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Equality of Opportunity (w.r.t y = 0)</td>\n",
       "      <td>0.970549</td>\n",
       "      <td>0.980068</td>\n",
       "      <td>0.962048</td>\n",
       "      <td>0.938017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Equality of Odds</td>\n",
       "      <td>0.958355</td>\n",
       "      <td>0.979097</td>\n",
       "      <td>0.939406</td>\n",
       "      <td>0.920706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unprotected-accuracy</td>\n",
       "      <td>0.868178</td>\n",
       "      <td>0.822829</td>\n",
       "      <td>0.721419</td>\n",
       "      <td>0.712221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>protected-accuracy</td>\n",
       "      <td>0.847538</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.720713</td>\n",
       "      <td>0.699491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.857858</td>\n",
       "      <td>0.814823</td>\n",
       "      <td>0.721066</td>\n",
       "      <td>0.705856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        fairness_metrics    gender  gender_DP      race  \\\n",
       "0                     demographic parity  0.998580   0.975852  0.898132   \n",
       "1  Equality of Opportunity (w.r.t y = 1)  0.946161   0.978125  0.916765   \n",
       "2  Equality of Opportunity (w.r.t y = 0)  0.970549   0.980068  0.962048   \n",
       "3                       Equality of Odds  0.958355   0.979097  0.939406   \n",
       "4                   unprotected-accuracy  0.868178   0.822829  0.721419   \n",
       "5                     protected-accuracy  0.847538   0.806818  0.720713   \n",
       "6                               accuracy  0.857858   0.814823  0.721066   \n",
       "\n",
       "    race_DP  \n",
       "0  0.797963  \n",
       "1  0.903395  \n",
       "2  0.938017  \n",
       "3  0.920706  \n",
       "4  0.712221  \n",
       "5  0.699491  \n",
       "6  0.705856  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_results = {\n",
    "   # make sure your calculate bias method returns bias metrics in the same order\n",
    "   'fairness_metrics': ['demographic parity', 'Equality of Opportunity (w.r.t y = 1)',\n",
    "'Equality of Opportunity (w.r.t y = 0)', 'Equality of Odds', 'unprotected-accuracy',\n",
    "'protected-accuracy', 'accuracy']\n",
    "}\n",
    "\n",
    "for group_key in group_map.keys():\n",
    "   subgroup_map = group_map[group_key]\n",
    "   privileged_group = subgroup_map['privileged']\n",
    "   unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "   bias_results[group_key] = calculate_bias(result, privileged_group, unprivileged_group)\n",
    "   bias_results[group_key+'_DP'] = calculate_bias(dp_result, privileged_group, unprivileged_group)\n",
    "\n",
    "bias_results = pd.DataFrame(bias_results) \n",
    "bias_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_results.round(3).to_csv(os.path.join(dp_folder, 'bias.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metrics</th>\n",
       "      <th>male</th>\n",
       "      <th>male_DP</th>\n",
       "      <th>female</th>\n",
       "      <th>female_DP</th>\n",
       "      <th>white</th>\n",
       "      <th>white_DP</th>\n",
       "      <th>black</th>\n",
       "      <th>black_DP</th>\n",
       "      <th>Total</th>\n",
       "      <th>Total_DP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>size</td>\n",
       "      <td>2112.000000</td>\n",
       "      <td>2112.000000</td>\n",
       "      <td>2602.000000</td>\n",
       "      <td>2602.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "      <td>761.000000</td>\n",
       "      <td>761.000000</td>\n",
       "      <td>97320.000000</td>\n",
       "      <td>97320.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auc</td>\n",
       "      <td>0.897062</td>\n",
       "      <td>0.849390</td>\n",
       "      <td>0.898539</td>\n",
       "      <td>0.850251</td>\n",
       "      <td>0.792065</td>\n",
       "      <td>0.771067</td>\n",
       "      <td>0.808122</td>\n",
       "      <td>0.737525</td>\n",
       "      <td>0.941883</td>\n",
       "      <td>0.894798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.847538</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>0.868178</td>\n",
       "      <td>0.822829</td>\n",
       "      <td>0.720713</td>\n",
       "      <td>0.699491</td>\n",
       "      <td>0.721419</td>\n",
       "      <td>0.712221</td>\n",
       "      <td>0.925390</td>\n",
       "      <td>0.909094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1_score</td>\n",
       "      <td>0.591371</td>\n",
       "      <td>0.531034</td>\n",
       "      <td>0.579141</td>\n",
       "      <td>0.515247</td>\n",
       "      <td>0.602177</td>\n",
       "      <td>0.602247</td>\n",
       "      <td>0.646667</td>\n",
       "      <td>0.598165</td>\n",
       "      <td>0.614494</td>\n",
       "      <td>0.553024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.497863</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.507527</td>\n",
       "      <td>0.407654</td>\n",
       "      <td>0.525316</td>\n",
       "      <td>0.499069</td>\n",
       "      <td>0.548023</td>\n",
       "      <td>0.545151</td>\n",
       "      <td>0.523332</td>\n",
       "      <td>0.455476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.728125</td>\n",
       "      <td>0.721875</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.705382</td>\n",
       "      <td>0.759207</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>0.662602</td>\n",
       "      <td>0.744117</td>\n",
       "      <td>0.703742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>false positive rate</td>\n",
       "      <td>0.131138</td>\n",
       "      <td>0.178013</td>\n",
       "      <td>0.101687</td>\n",
       "      <td>0.158082</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.326061</td>\n",
       "      <td>0.310680</td>\n",
       "      <td>0.264078</td>\n",
       "      <td>0.058866</td>\n",
       "      <td>0.073071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bnsp_auc</td>\n",
       "      <td>0.951126</td>\n",
       "      <td>0.927555</td>\n",
       "      <td>0.942339</td>\n",
       "      <td>0.920170</td>\n",
       "      <td>0.947406</td>\n",
       "      <td>0.918442</td>\n",
       "      <td>0.955836</td>\n",
       "      <td>0.879579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bpsn_auc</td>\n",
       "      <td>0.888014</td>\n",
       "      <td>0.813385</td>\n",
       "      <td>0.902654</td>\n",
       "      <td>0.824310</td>\n",
       "      <td>0.812069</td>\n",
       "      <td>0.751548</td>\n",
       "      <td>0.792129</td>\n",
       "      <td>0.779186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               metrics         male      male_DP       female    female_DP  \\\n",
       "0                 size  2112.000000  2112.000000  2602.000000  2602.000000   \n",
       "1                  auc     0.897062     0.849390     0.898539     0.850251   \n",
       "2             accuracy     0.847538     0.806818     0.868178     0.822829   \n",
       "3             f1_score     0.591371     0.531034     0.579141     0.515247   \n",
       "4            precision     0.497863     0.420000     0.507527     0.407654   \n",
       "5               recall     0.728125     0.721875     0.674286     0.700000   \n",
       "6  false positive rate     0.131138     0.178013     0.101687     0.158082   \n",
       "7             bnsp_auc     0.951126     0.927555     0.942339     0.920170   \n",
       "8             bpsn_auc     0.888014     0.813385     0.902654     0.824310   \n",
       "\n",
       "         white     white_DP       black    black_DP         Total  \\\n",
       "0  1178.000000  1178.000000  761.000000  761.000000  97320.000000   \n",
       "1     0.792065     0.771067    0.808122    0.737525      0.941883   \n",
       "2     0.720713     0.699491    0.721419    0.712221      0.925390   \n",
       "3     0.602177     0.602247    0.646667    0.598165      0.614494   \n",
       "4     0.525316     0.499069    0.548023    0.545151      0.523332   \n",
       "5     0.705382     0.759207    0.788618    0.662602      0.744117   \n",
       "6     0.272727     0.326061    0.310680    0.264078      0.058866   \n",
       "7     0.947406     0.918442    0.955836    0.879579           NaN   \n",
       "8     0.812069     0.751548    0.792129    0.779186           NaN   \n",
       "\n",
       "       Total_DP  \n",
       "0  97320.000000  \n",
       "1      0.894798  \n",
       "2      0.909094  \n",
       "3      0.553024  \n",
       "4      0.455476  \n",
       "5      0.703742  \n",
       "6      0.073071  \n",
       "7           NaN  \n",
       "8           NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results = {\n",
    "    'metrics': ['size', 'auc', 'accuracy', 'f1_score', \n",
    "    'precision', 'recall', 'false positive rate',\n",
    "    'bnsp_auc', 'bpsn_auc']\n",
    "}\n",
    "\n",
    "for group_key in group_map.keys():\n",
    "    subgroup_map = group_map[group_key]\n",
    "    privileged_group = subgroup_map['privileged']\n",
    "    unprivileged_group = subgroup_map['unprivileged']\n",
    "\n",
    "    privileged_group_name = ','.join(privileged_group)\n",
    "    unprivileged_group_name = ','.join(unprivileged_group)\n",
    "\n",
    "    overall_results[privileged_group_name] = calculate_metrics(result, privileged_group)\n",
    "    overall_results[privileged_group_name + '_DP'] = calculate_metrics(dp_result, privileged_group)\n",
    "\n",
    "    overall_results[unprivileged_group_name] = calculate_metrics(result, unprivileged_group)\n",
    "    overall_results[unprivileged_group_name + '_DP'] = calculate_metrics(dp_result, unprivileged_group)\n",
    "\n",
    "overall_results['Total'] = calculate_metrics(result, [])\n",
    "overall_results['Total_DP'] = calculate_metrics(dp_result, [])\n",
    "\n",
    "overall_results = pd.DataFrame(overall_results) \n",
    "overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results.round(3).to_csv(os.path.join(dp_folder, 'overall_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall bias auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc = overall_results[overall_results['metrics']=='auc']['Total'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8927666570365307 0.8934576301082992 0.8620616692527434 0.8555933902575662\n"
     ]
    }
   ],
   "source": [
    "temp = overall_results.set_index('metrics').T\n",
    "privileged, unprivileged = [], []\n",
    "for group_key in group_map.keys():\n",
    "    privileged.extend(group_map[group_key]['privileged'])\n",
    "    unprivileged.extend(group_map[group_key]['unprivileged'])\n",
    "\n",
    "privileged_bias_auc = get_final_metric(temp[temp.index.isin(privileged)], overall_auc)\n",
    "unprivileged_bias_auc = get_final_metric(temp[temp.index.isin(unprivileged)], overall_auc)\n",
    "\n",
    "privileged_bias_auc_dp = get_final_metric(\n",
    "    temp[temp.index.isin([i+'_DP' for i in privileged])], \n",
    "    overall_auc\n",
    ")\n",
    "\n",
    "unprivileged_bias_auc_dp = get_final_metric(\n",
    "    temp[temp.index.isin([i+'_DP' for i in unprivileged])], \n",
    "    overall_auc\n",
    ")\n",
    "print(privileged_bias_auc, unprivileged_bias_auc, privileged_bias_auc_dp, unprivileged_bias_auc_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count\n",
    "The the `train.csv` file from `experiment/run` folders for this corresponding run. And manually copy it in the path run_folder points to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(run_folder, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Identity  0 (train)  1 (train)  0 (test)  1 (test)\n",
      "0   female      36869       5855      4463       692\n",
      "1     male      30123       5386      3716       670\n",
      "2    black       8260       3748      1016       503\n",
      "3    white      14473       5683      1710       742\n"
     ]
    }
   ],
   "source": [
    "count_dict = {\n",
    "    'Identity':identities,\n",
    "    '0 (train)':[],\n",
    "    '1 (train)':[],\n",
    "    '0 (test)':[],\n",
    "    '1 (test)':[],\n",
    "}\n",
    "for identity in identities:\n",
    "    train_neg, train_pos = train[train[identity]>=0.5][target_column].value_counts().to_numpy()\n",
    "    test_neg, test_pos = df[df[identity]>=0.5][target_column].value_counts().to_numpy()\n",
    "    count_dict['0 (train)'].append(train_neg)\n",
    "    count_dict['1 (train)'].append(train_pos)\n",
    "    count_dict['0 (test)'].append(test_neg)\n",
    "    count_dict['1 (test)'].append(test_pos)\n",
    "\n",
    "count_df = pd.DataFrame(count_dict)\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.to_csv(os.path.join(run_folder, 'count.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

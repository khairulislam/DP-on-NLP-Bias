{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import sys\n",
    "# this adds the src folder in the sys path, where the metric_utils.py file is\n",
    "# not needed if this notebook is in the same folder, but uncomment to access from the data subfolders\n",
    "sys.path.append( '..' )\n",
    "from metric_utils import *\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'jigsaw'\n",
    "model_name = 'bert-base-uncased'\n",
    "run_name = \"run 1\"\n",
    "\n",
    "# change to f'../../results/{dataset_name}' when using inside one of the data subfolders\n",
    "run_folder = f'../../results/{dataset_name}/{run_name}'\n",
    "test_csv_filepath = os.path.join(f'../../results/{dataset_name}', 'test.csv')\n",
    "\n",
    "model_folder = os.path.join(run_folder, model_name) # for this particular model\n",
    "result_folder = os.path.join(model_folder, 'normal')\n",
    "\n",
    "result_filepath = os.path.join(result_folder, 'results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset can be preprocessed from the original dataset to be used here. To simplify things, I saved the preprocessed datasets during the tokenizing process as csv files and then downloaded them in the corresponding dataset folder of [`result`](../results/) directory.\n",
    "\n",
    "You can recreate the processed datasets using the tokenize notebooks for that particular dataset. That would give you train, test and validation csv files as well as the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>labels</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>black</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>christian</th>\n",
       "      <th>jewish</th>\n",
       "      <th>muslim</th>\n",
       "      <th>hindu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7084460</td>\n",
       "      <td>\"while arresting a man for resisting arrest\".\\n\\nIf you cop-suckers can't see a problem with this, then go suck the barrel of a Glock.</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7141509</td>\n",
       "      <td>NO !  There are no alternative facts. Go check for yourself. It is people like you, who have no idea what you are talking about that has gotten this State and Country into the mess it is in. People who think the Goverment, be it State or Federal, can spend the peoples money better than they can, is stupid and nonsensical. Politicians use taxes as Personal slush accounts to continue their carrers, buying votes from the lame and the lazy.</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  \\\n",
       "0  7084460   \n",
       "1  7141509   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                               comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                    \"while arresting a man for resisting arrest\".\\n\\nIf you cop-suckers can't see a problem with this, then go suck the barrel of a Glock.   \n",
       "1  NO !  There are no alternative facts. Go check for yourself. It is people like you, who have no idea what you are talking about that has gotten this State and Country into the mess it is in. People who think the Goverment, be it State or Federal, can spend the peoples money better than they can, is stupid and nonsensical. Politicians use taxes as Personal slush accounts to continue their carrers, buying votes from the lame and the lazy.   \n",
       "\n",
       "   labels  male  female  transgender  white  black  asian  latino  \\\n",
       "0    True   NaN     NaN          NaN    NaN    NaN    NaN     NaN   \n",
       "1    True   NaN     NaN          NaN    NaN    NaN    NaN     NaN   \n",
       "\n",
       "   heterosexual  homosexual_gay_or_lesbian  bisexual  christian  jewish  \\\n",
       "0           NaN                        NaN       NaN        NaN     NaN   \n",
       "1           NaN                        NaN       NaN        NaN     NaN   \n",
       "\n",
       "   muslim  hindu  \n",
       "0     NaN    NaN  \n",
       "1     NaN    NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv(result_filepath)\n",
    "test_df = pd.read_csv(test_csv_filepath)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing this reduces the test file size, makes things easier with git\n",
    "if 'comment_text' in test_df.columns:\n",
    "    test_df.drop(columns='comment_text', inplace=True)\n",
    "    test_df.to_csv(test_csv_filepath, index=False)\n",
    "\n",
    "test_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male False    3716\n",
      "True      670\n",
      "Name: labels, dtype: int64\n",
      "female False    4463\n",
      "True      692\n",
      "Name: labels, dtype: int64\n",
      "transgender False    209\n",
      "True      51\n",
      "Name: labels, dtype: int64\n",
      "white False    1710\n",
      "True      742\n",
      "Name: labels, dtype: int64\n",
      "black False    1016\n",
      "True      503\n",
      "Name: labels, dtype: int64\n",
      "asian False    408\n",
      "True      46\n",
      "Name: labels, dtype: int64\n",
      "latino False    174\n",
      "True      51\n",
      "Name: labels, dtype: int64\n",
      "heterosexual False    112\n",
      "True      29\n",
      "Name: labels, dtype: int64\n",
      "homosexual_gay_or_lesbian False    775\n",
      "True     290\n",
      "Name: labels, dtype: int64\n",
      "bisexual False    27\n",
      "True      7\n",
      "Name: labels, dtype: int64\n",
      "christian False    3809\n",
      "True      417\n",
      "Name: labels, dtype: int64\n",
      "jewish False    697\n",
      "True     138\n",
      "Name: labels, dtype: int64\n",
      "muslim False    1557\n",
      "True      483\n",
      "Name: labels, dtype: int64\n",
      "hindu False    46\n",
      "True      5\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "identities = ['male', 'female', 'transgender', 'white', 'black', 'asian']\n",
    "for identity in identities:\n",
    "    print(identity, test_df[test_df[identity]>=0.5][target_column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only calculate test result\n",
    "result = result[result['split']=='test']\n",
    "\n",
    "# drop split column\n",
    "result.drop(columns=['split'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_columns = [col for col in test_df.columns if col in result.columns and col!=id_column]\n",
    "result = result.merge(test_df.drop(columns=extra_columns), on=id_column, how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert probability to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[prediction_column] = result[probability_column] >=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_map = {\n",
    "    'gender': {\n",
    "        'unprivileged':['female'],\n",
    "        'privileged':['male']\n",
    "    },\n",
    "    'race': {\n",
    "        'unprivileged':['black'],\n",
    "        'privileged': ['white']\n",
    "    }\n",
    "}\n",
    "\n",
    "identities = []\n",
    "for group_key in group_map.keys():\n",
    "    subgroup_map = group_map[group_key]\n",
    "    for subgroup_key in subgroup_map.keys():\n",
    "        identities.extend(subgroup_map[subgroup_key])\n",
    "\n",
    "print(identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize identity and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = binarize(result, [target_column] + identities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_results = get_all_bias(group_map, result)\n",
    "bias_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_results.round(3).to_csv(os.path.join(model_folder, 'bias.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = get_overall_results(group_map, result)\n",
    "overall_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results.round(3).to_csv(os.path.join(model_folder, 'overall_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall bias auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_auc = overall_results[overall_results['metrics']=='auc']['Total'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = overall_results.set_index('metrics').T\n",
    "privileged, unprivileged = [], []\n",
    "for group_key in group_map.keys():\n",
    "    privileged.extend(group_map[group_key]['privileged'])\n",
    "    unprivileged.extend(group_map[group_key]['unprivileged'])\n",
    "\n",
    "privileged_bias_auc = get_final_metric(temp[temp.index.isin(privileged)], overall_auc)\n",
    "unprivileged_bias_auc = get_final_metric(temp[temp.index.isin(unprivileged)], overall_auc)\n",
    "\n",
    "print(privileged_bias_auc, unprivileged_bias_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count\n",
    "The the `train.csv` file from `experiment/run` folders for this corresponding run. And manually copy it in the path run_folder points to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(run_folder, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dict = {\n",
    "    'Identity':identities,\n",
    "    '0 (train)':[],\n",
    "    '1 (train)':[],\n",
    "    '0 (test)':[],\n",
    "    '1 (test)':[],\n",
    "}\n",
    "for identity in identities:\n",
    "    train_neg, train_pos = train[train[identity]>=0.5][target_column].value_counts().to_numpy()\n",
    "    test_neg, test_pos = test_df[test_df[identity]>=0.5][target_column].value_counts().to_numpy()\n",
    "    count_dict['0 (train)'].append(train_neg)\n",
    "    count_dict['1 (train)'].append(train_pos)\n",
    "    count_dict['0 (test)'].append(test_neg)\n",
    "    count_dict['1 (test)'].append(test_pos)\n",
    "\n",
    "count_df = pd.DataFrame(count_dict)\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.to_csv(os.path.join(run_folder, 'count.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

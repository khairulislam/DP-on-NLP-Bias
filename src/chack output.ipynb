{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from metric_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "epsilon_list = sorted([0.5, 1.0, 3.0, 6.0, 9.0])[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_map = {\n",
    "    'jigsaw': ['male', 'female', 'transgender', 'white', 'black', 'asian'],\n",
    "    'ucberkeley': ['target_gender_men', 'target_gender_women','target_gender_transgender', 'target_race_white', 'target_race_black', 'target_race_asian']\n",
    "}\n",
    "\n",
    "for dataset_name in ['jigsaw', 'ucberkeley']:\n",
    "    print(dataset_name)\n",
    "    dataset_directory = f'../results/{dataset_name}/'\n",
    "    avg_result = {}\n",
    "    protected_subgroups = subgroup_map[dataset_name]\n",
    "    binarizing_columns = [target_column] + protected_subgroups\n",
    "\n",
    "    for run in range(1, 4):\n",
    "        run_folder = f'{dataset_directory}/run {run}'\n",
    "        model_folder = os.path.join(run_folder, model_name)\n",
    "        normal_folder = os.path.join(model_folder, 'normal')\n",
    "        result_filepath = os.path.join(normal_folder, 'results.csv')\n",
    "\n",
    "        result = pd.read_csv(result_filepath)\n",
    "        result = result[result['split']=='test']\n",
    "        # drop split column\n",
    "        result.drop(columns=['split'], inplace=True)\n",
    "\n",
    "        if dataset_name=='ucberkeley':\n",
    "            test_csv_filepath = os.path.join(run_folder, 'test.csv')\n",
    "        else:\n",
    "            test_csv_filepath = os.path.join(dataset_directory, 'test.csv')\n",
    "\n",
    "        test_df = pd.read_csv(test_csv_filepath)\n",
    "\n",
    "        test_df.fillna(0, inplace=True)\n",
    "        # result has id column which is the same as the text ids from raw dataset\n",
    "        \n",
    "        if dataset_name=='ucberkeley':\n",
    "            test_df.rename({'comment_id': id_column}, axis=1, inplace=True)\n",
    "\n",
    "        # if test df has any common columns except id, drop that during merge\n",
    "        extra_columns = [col for col in test_df.columns if col in result.columns and col!=id_column]\n",
    "\n",
    "        result = result.merge(test_df.drop(columns=extra_columns), on=id_column, how='inner').reset_index(drop=True)\n",
    "        result[prediction_column] = result[probability_column] >=0.5\n",
    "        result = binarize(result, binarizing_columns)\n",
    "\n",
    "        if run ==1:\n",
    "            avg_result['None_total'] = [result[prediction_column].value_counts(normalize=True)]\n",
    "            for group in protected_subgroups:\n",
    "                avg_result[f'None_{group}'] = [result[result[group]][prediction_column].value_counts(normalize=True)]\n",
    "        else:\n",
    "            avg_result['None_total'].append(result[prediction_column].value_counts(normalize=True))\n",
    "            for group in protected_subgroups:\n",
    "                    avg_result[f'None_{group}'].append(result[result[group]][prediction_column].value_counts(normalize=True))\n",
    "\n",
    "        for epsilon in sorted(epsilon_list)[::-1]:\n",
    "        \n",
    "            dp_folder = os.path.join(model_folder, f'epsilon {epsilon}')\n",
    "            dp_result_filepath = os.path.join(dp_folder, 'results.csv')\n",
    "            dp_result = pd.read_csv(dp_result_filepath)\n",
    "\n",
    "            # only calculate test result\n",
    "            dp_result = dp_result[dp_result['split']=='test']\n",
    "            dp_result = dp_result.merge(test_df.drop(columns=extra_columns), on=id_column, how='inner').reset_index(drop=True)\n",
    "        \n",
    "            dp_result[prediction_column] = dp_result[probability_column] >=0.5\n",
    "            dp_result = binarize(dp_result, binarizing_columns)\n",
    "\n",
    "            if run ==1:\n",
    "                avg_result[f'{epsilon}_total'] = [dp_result[prediction_column].value_counts(normalize=True)]\n",
    "                for group in protected_subgroups:\n",
    "                    avg_result[f'{epsilon}_{group}'] = [dp_result[dp_result[group]][prediction_column].value_counts(normalize=True)]\n",
    "            else:\n",
    "                avg_result[f'{epsilon}_total'].append(dp_result[prediction_column].value_counts(normalize=True))\n",
    "                for group in protected_subgroups:\n",
    "                    avg_result[f'{epsilon}_{group}'].append(dp_result[dp_result[group]][prediction_column].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "    for key in avg_result.keys():\n",
    "        avg_result[key] = pd.concat(avg_result[key]).reset_index()\n",
    "        mean = avg_result[key].groupby('index').agg('mean').reset_index()\n",
    "        print(key)\n",
    "        print(mean)\n",
    "        print()\n",
    "    print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82c0d4b75d1a522b549257adf6e3ea321f1ee050a595ab76efcf522f2572b2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

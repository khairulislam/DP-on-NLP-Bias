{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is done following \n* [Building text classifier with Differential Privacy](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)\n* [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.4.0/custom_datasets.html#seq-imdb)","metadata":{"id":"huyLXVRDUwFw"}},{"cell_type":"markdown","source":"# Intial Setup\nhttps://huggingface.co/docs/transformers/training","metadata":{"id":"n3CQPh6pRJpP"}},{"cell_type":"markdown","source":"## Install","metadata":{"id":"tB5WsXAHyBZv"}},{"cell_type":"code","source":"!pip install opacus\n# !pip install transformers\n!pip install datasets\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:24:33.502154Z","iopub.execute_input":"2022-06-06T01:24:33.502504Z","iopub.status.idle":"2022-06-06T01:25:07.09282Z","shell.execute_reply.started":"2022-06-06T01:24:33.502395Z","shell.execute_reply":"2022-06-06T01:25:07.091738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{"id":"nxhRYG1UyFLJ"}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom opacus.utils.batch_memory_manager import BatchMemoryManager\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\npd.set_option('display.max_columns', None)","metadata":{"id":"MjZvbCbYx-no","execution":{"iopub.status.busy":"2022-06-06T01:25:07.103185Z","iopub.execute_input":"2022-06-06T01:25:07.106042Z","iopub.status.idle":"2022-06-06T01:25:07.927126Z","shell.execute_reply.started":"2022-06-06T01:25:07.105979Z","shell.execute_reply":"2022-06-06T01:25:07.926127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:07.933186Z","iopub.execute_input":"2022-06-06T01:25:07.936452Z","iopub.status.idle":"2022-06-06T01:25:07.957796Z","shell.execute_reply.started":"2022-06-06T01:25:07.936382Z","shell.execute_reply":"2022-06-06T01:25:07.956773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass Config:\n    # train config\n    model_name = 'bert-base-uncased'\n    batch_size = 64\n    learning_rate = 1e-4\n    epochs = 20\n    num_labels = 2\n\n    dataset_name = 'social_bias_frames'\n    text_column = 'post'\n    # if id column is string, replace that with an integer index during preprocessing\n    raw_id_column = 'HITId'\n    id_column = 'index'\n\n    # the original id column HITId has been replaced with index because it was string \n    # and torch didn't support str format\n    raw_id_column = 'HITId'\n    id_column = 'index'\n\n    # Private training config\n    delta_list = [5e-2, 1e-3, 1e-5]\n    noise_multiplier = 0.4\n    max_grad_norm = 1\n    max_physical_batch_size = 32\n\n    # target in raw dataset. However, it will be renamed to `labels` here to facilitate training setup\n    raw_target_column = 'offensiveYN'\n    target_column = 'labels'\n\n    # If needs to be splitted into train test validation set\n    need_to_split = False\n    # test and validation data with each be 50% of this amount\n    test_size = 0.3\n    max_seq_length = 128\n    seed = 2022","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:07.962242Z","iopub.execute_input":"2022-06-06T01:25:07.96294Z","iopub.status.idle":"2022-06-06T01:25:07.972872Z","shell.execute_reply.started":"2022-06-06T01:25:07.962894Z","shell.execute_reply":"2022-06-06T01:25:07.971597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set seed","metadata":{}},{"cell_type":"code","source":"import random\n\ndef seed_torch(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\nglobal_seed = Config.seed\nseed_torch(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:07.974952Z","iopub.execute_input":"2022-06-06T01:25:07.97564Z","iopub.status.idle":"2022-06-06T01:25:07.987438Z","shell.execute_reply.started":"2022-06-06T01:25:07.97557Z","shell.execute_reply":"2022-06-06T01:25:07.986373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get device","metadata":{"id":"CSnXCK2k6KvV"}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"id":"N01plY896Mps","outputId":"3d0d33d9-0b02-4ad8-c8c0-b227b57bf1fb","execution":{"iopub.status.busy":"2022-06-06T01:25:07.992461Z","iopub.execute_input":"2022-06-06T01:25:07.9932Z","iopub.status.idle":"2022-06-06T01:25:08.060812Z","shell.execute_reply.started":"2022-06-06T01:25:07.993135Z","shell.execute_reply":"2022-06-06T01:25:08.059505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load tokenized data\n\nFrom my [other notebook](https://www.kaggle.com/code/khairulislam/tokenize-jigsaw-comments). The dataset is tokenized from the [Jigsaw competition]( https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) and [all_data.csv](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv)","metadata":{}},{"cell_type":"code","source":"text = Config.text_column\ntarget = Config.target_column\nroot = '/kaggle/input/tokenize-social-bias-using-bert/'","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:08.06224Z","iopub.execute_input":"2022-06-06T01:25:08.064685Z","iopub.status.idle":"2022-06-06T01:25:08.072047Z","shell.execute_reply.started":"2022-06-06T01:25:08.06436Z","shell.execute_reply":"2022-06-06T01:25:08.070698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n    \nwith open(root + 'train.pkl', 'rb') as input_file:\n    train_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'validation.pkl', 'rb') as input_file:\n    validation_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'test.pkl', 'rb') as input_file:\n    test_tokenized = pickle.load(input_file)\n    input_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:08.073409Z","iopub.execute_input":"2022-06-06T01:25:08.073747Z","iopub.status.idle":"2022-06-06T01:25:08.33775Z","shell.execute_reply.started":"2022-06-06T01:25:08.073642Z","shell.execute_reply":"2022-06-06T01:25:08.336876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_tokenized)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:08.339262Z","iopub.execute_input":"2022-06-06T01:25:08.339561Z","iopub.status.idle":"2022-06-06T01:25:08.349931Z","shell.execute_reply.started":"2022-06-06T01:25:08.339521Z","shell.execute_reply":"2022-06-06T01:25:08.348863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove id column from the data to be batched\nid_column = Config.id_column \ntrain_tokenized = train_tokenized.remove_columns(id_column)\ntest_tokenized = test_tokenized.remove_columns(id_column)\nvalidation_tokenized = validation_tokenized.remove_columns(id_column)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:08.355933Z","iopub.execute_input":"2022-06-06T01:25:08.356213Z","iopub.status.idle":"2022-06-06T01:25:08.369938Z","shell.execute_reply.started":"2022-06-06T01:25:08.356172Z","shell.execute_reply":"2022-06-06T01:25:08.369084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Private Training","metadata":{"id":"Cf0mMuDWNQla"}},{"cell_type":"markdown","source":"## Data loader\n\n[How to choose batch size in DP](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = Config.batch_size\n\ntrain_dataloader = DataLoader(train_tokenized, batch_size=BATCH_SIZE)\nvalidation_dataloader = DataLoader(validation_tokenized, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_tokenized, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:08.371398Z","iopub.execute_input":"2022-06-06T01:25:08.371831Z","iopub.status.idle":"2022-06-06T01:25:08.379751Z","shell.execute_reply.started":"2022-06-06T01:25:08.371787Z","shell.execute_reply":"2022-06-06T01:25:08.378663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model and optimizer","metadata":{}},{"cell_type":"code","source":"# add the utility script from File->Add utility script\nfrom train_utils import TrainUtil, ModelCheckPoint, EarlyStopping\n\nnum_labels = Config.num_labels\nmodel_name = Config.model_name\ntrain_util = TrainUtil(Config.id_column, Config.target_column, device)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:08.381814Z","iopub.execute_input":"2022-06-06T01:25:08.382668Z","iopub.status.idle":"2022-06-06T01:25:08.49395Z","shell.execute_reply.started":"2022-06-06T01:25:08.382585Z","shell.execute_reply":"2022-06-06T01:25:08.493085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load a fresh model each time\nmodel = TrainUtil.load_pretrained_model(model_name, num_labels)\n\n# Set the model to train mode (HuggingFace models load in eval mode)\nmodel = model.train().to(device)\nLEARNING_RATE = Config.learning_rate\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\nEPOCHS = Config.epochs\n\n# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, verbose=True)","metadata":{"id":"2VJx_xjQNTaZ","execution":{"iopub.status.busy":"2022-06-06T01:25:08.495542Z","iopub.execute_input":"2022-06-06T01:25:08.495875Z","iopub.status.idle":"2022-06-06T01:25:58.837998Z","shell.execute_reply.started":"2022-06-06T01:25:08.495833Z","shell.execute_reply":"2022-06-06T01:25:58.837185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_type = model_name.split(r'/')[-1]\nresult_dir = '' # f'dp models/{model_type}/{NOISE_MULTIPLIER*10}/'\nbest_model_path = os.path.join(result_dir, 'model.pt')\n\nif result_dir != '':\n    os.makedirs(result_dir, exist_ok=True)\n\ncheck_point = ModelCheckPoint(filepath=best_model_path)\nearly_stopping = EarlyStopping(patience=3, min_delta=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:25:58.839535Z","iopub.execute_input":"2022-06-06T01:25:58.83984Z","iopub.status.idle":"2022-06-06T01:25:58.846811Z","shell.execute_reply.started":"2022-06-06T01:25:58.839799Z","shell.execute_reply":"2022-06-06T01:25:58.845725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Privacy Engine","metadata":{"id":"b7EFOaY2NeiR"}},{"cell_type":"code","source":"from opacus import PrivacyEngine\n\nprivacy_engine = PrivacyEngine()","metadata":{"id":"R00MPKNsh205","execution":{"iopub.status.busy":"2022-06-06T01:25:58.848851Z","iopub.execute_input":"2022-06-06T01:25:58.849237Z","iopub.status.idle":"2022-06-06T01:25:58.858572Z","shell.execute_reply.started":"2022-06-06T01:25:58.849179Z","shell.execute_reply":"2022-06-06T01:25:58.857337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, optimizer, train_dataloader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=train_dataloader,\n    noise_multiplier=Config.noise_multiplier,\n    max_grad_norm=Config.max_grad_norm,\n    poisson_sampling=False,\n)","metadata":{"id":"Wta4qx6mNgqB","execution":{"iopub.status.busy":"2022-06-06T01:25:58.861058Z","iopub.execute_input":"2022-06-06T01:25:58.861503Z","iopub.status.idle":"2022-06-06T01:25:58.88075Z","shell.execute_reply.started":"2022-06-06T01:25:58.861433Z","shell.execute_reply":"2022-06-06T01:25:58.879836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loop","metadata":{}},{"cell_type":"code","source":"start_epoch = 1\n# load a previous model if there is any\n# model, optimizer, lr_scheduler, start_epoch = load_model(model, optimizer, lr_scheduler, device, filepath=best_model_path)\n\nfor epoch in range(start_epoch, EPOCHS+1):\n    gc.collect()\n    \n    with BatchMemoryManager(\n        data_loader=train_dataloader, \n        max_physical_batch_size=Config.max_physical_batch_size, \n        optimizer=optimizer\n    ) as memory_safe_data_loader:\n        train_loss, train_result, train_probs = train_util.dp_train(\n            model, optimizer, epoch, memory_safe_data_loader\n        )\n    val_loss, val_result, val_probs = train_util.evaluate(\n        model, validation_dataloader, epoch, 'Validation'\n    )\n\n    epsilons = []\n    for delta in Config.delta_list:\n        epsilons.append(privacy_engine.get_epsilon(delta))\n\n    print(\n      f\"Epoch: {epoch} | \"\n      f\"É›: {np.round(epsilons, 2)} |\"\n      f\"Train loss: {train_loss:.3f} | \"\n      f\"Train result: {train_result} |\\n\"\n      f\"Validation loss: {val_loss:.3f} | \"\n      f\"Validation result: {val_result} | \"\n    )\n    \n    loss = -val_result['f1']\n    lr_scheduler.step(loss)\n    check_point(model, optimizer, lr_scheduler, epoch, loss)\n    \n    early_stopping(loss)\n    if early_stopping.early_stop:\n        break\n    print()\n    # break","metadata":{"id":"RkZPY4iIUUOw","outputId":"aebf8df4-0009-4678-b0d5-392753e178e9","execution":{"iopub.status.busy":"2022-06-06T01:33:31.688365Z","iopub.execute_input":"2022-06-06T01:33:31.688727Z","iopub.status.idle":"2022-06-06T02:02:20.080024Z","shell.execute_reply.started":"2022-06-06T01:33:31.688688Z","shell.execute_reply":"2022-06-06T02:02:20.078961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the best model\nmodel, _, _, best_epoch = TrainUtil.load_model(model, optimizer, lr_scheduler, device, filepath=best_model_path)\n\ntrain_loss, train_result, train_probs = train_util.evaluate(model, train_dataloader, best_epoch, 'Train')\n# no need to reevaluate if the validation set if the last model is the best one\nif best_epoch != epoch:\n    val_loss, val_result, val_probs = train_util.evaluate(model, validation_dataloader, best_epoch, 'Validation')\ntest_loss, test_result, test_probs = train_util.evaluate(model, test_dataloader, best_epoch, 'Test')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:29:53.360458Z","iopub.execute_input":"2022-06-06T01:29:53.361272Z","iopub.status.idle":"2022-06-06T01:32:42.152971Z","shell.execute_reply.started":"2022-06-06T01:29:53.36123Z","shell.execute_reply":"2022-06-06T01:32:42.152188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dump results and others","metadata":{}},{"cell_type":"code","source":"# load the original tokenized files, since we removed the id columns earlier\n# and id columns are needed for the result dumping part\nwith open(root + 'train.pkl', 'rb') as input_file:\n    train_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'validation.pkl', 'rb') as input_file:\n    validation_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'test.pkl', 'rb') as input_file:\n    test_tokenized = pickle.load(input_file)\n    input_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:32:42.154494Z","iopub.execute_input":"2022-06-06T01:32:42.154805Z","iopub.status.idle":"2022-06-06T01:32:42.190526Z","shell.execute_reply.started":"2022-06-06T01:32:42.154765Z","shell.execute_reply":"2022-06-06T01:32:42.189732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the results\ntrain_util.dump_results(\n    result_dir,train_probs, train_tokenized, \n    val_probs, validation_tokenized, test_probs, test_tokenized\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:32:42.192022Z","iopub.execute_input":"2022-06-06T01:32:42.192532Z","iopub.status.idle":"2022-06-06T01:32:42.398735Z","shell.execute_reply.started":"2022-06-06T01:32:42.19249Z","shell.execute_reply":"2022-06-06T01:32:42.397877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save config","metadata":{}},{"cell_type":"code","source":"import json\n\nconfig_dict = dict(Config.__dict__)\n# exclude hidden variables\nkeys = list(config_dict.keys())\nfor key in keys:\n    if key.startswith('__'):\n        del config_dict[key]\n        \nwith open(os.path.join(result_dir, 'config.json'), 'w') as output:\n    json.dump(config_dict, output, indent=4)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T01:33:20.244489Z","iopub.execute_input":"2022-06-06T01:33:20.245003Z","iopub.status.idle":"2022-06-06T01:33:20.252162Z","shell.execute_reply.started":"2022-06-06T01:33:20.244961Z","shell.execute_reply":"2022-06-06T01:33:20.251288Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
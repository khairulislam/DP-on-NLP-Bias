{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is done following \n* [Building text classifier with Differential Privacy](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)\n* [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.4.0/custom_datasets.html#seq-imdb)","metadata":{"id":"huyLXVRDUwFw"}},{"cell_type":"markdown","source":"# Libraries\nhttps://huggingface.co/docs/transformers/training","metadata":{"id":"n3CQPh6pRJpP"}},{"cell_type":"markdown","source":"## Install","metadata":{"id":"tB5WsXAHyBZv"}},{"cell_type":"code","source":"!pip install opacus","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:17:15.597299Z","iopub.execute_input":"2022-05-07T15:17:15.597599Z","iopub.status.idle":"2022-05-07T15:17:30.487947Z","shell.execute_reply.started":"2022-05-07T15:17:15.597517Z","shell.execute_reply":"2022-05-07T15:17:30.486764Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:18:25.828833Z","iopub.execute_input":"2022-05-07T15:18:25.829473Z","iopub.status.idle":"2022-05-07T15:18:40.239216Z","shell.execute_reply.started":"2022-05-07T15:18:25.829437Z","shell.execute_reply":"2022-05-07T15:18:40.237934Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{"id":"nxhRYG1UyFLJ"}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom opacus.utils.batch_memory_manager import BatchMemoryManager\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\npd.set_option('display.max_columns', None)","metadata":{"id":"MjZvbCbYx-no","execution":{"iopub.status.busy":"2022-05-07T15:17:30.492466Z","iopub.execute_input":"2022-05-07T15:17:30.492756Z","iopub.status.idle":"2022-05-07T15:17:37.750185Z","shell.execute_reply.started":"2022-05-07T15:17:30.492707Z","shell.execute_reply":"2022-05-07T15:17:37.749202Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:17:46.319781Z","iopub.execute_input":"2022-05-07T15:17:46.320130Z","iopub.status.idle":"2022-05-07T15:17:46.338318Z","shell.execute_reply.started":"2022-05-07T15:17:46.320099Z","shell.execute_reply":"2022-05-07T15:17:46.336483Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef seed_torch(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\nglobal_seed = 2022\nseed_torch(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:17:58.145301Z","iopub.execute_input":"2022-05-07T15:17:58.145621Z","iopub.status.idle":"2022-05-07T15:17:58.157367Z","shell.execute_reply.started":"2022-05-07T15:17:58.145591Z","shell.execute_reply":"2022-05-07T15:17:58.156265Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Get device","metadata":{"id":"CSnXCK2k6KvV"}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"id":"N01plY896Mps","outputId":"3d0d33d9-0b02-4ad8-c8c0-b227b57bf1fb","execution":{"iopub.status.busy":"2022-05-07T15:18:00.239863Z","iopub.execute_input":"2022-05-07T15:18:00.240346Z","iopub.status.idle":"2022-05-07T15:18:00.306570Z","shell.execute_reply.started":"2022-05-07T15:18:00.240302Z","shell.execute_reply":"2022-05-07T15:18:00.305216Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Load tokenized data\n\nFrom my [other notebook](https://www.kaggle.com/code/khairulislam/tokenize-jigsaw-comments). The dataset is tokenized from the [Jigsaw competition]( https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) and [all_data.csv](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv)","metadata":{}},{"cell_type":"code","source":"text = 'comment_text'\ntarget = 'labels'\nroot = '/kaggle/input/tokenize-jigsaw-comments/'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:18:05.670943Z","iopub.execute_input":"2022-05-07T15:18:05.671311Z","iopub.status.idle":"2022-05-07T15:18:05.678529Z","shell.execute_reply.started":"2022-05-07T15:18:05.671280Z","shell.execute_reply":"2022-05-07T15:18:05.677065Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pickle\n    \nwith open(root + 'test.pkl', 'rb') as input_file:\n    test_all_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'train_undersampled.pkl', 'rb') as input_file:\n    train_all_tokenized = pickle.load(input_file)\n    input_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:18:45.871841Z","iopub.execute_input":"2022-05-07T15:18:45.872193Z","iopub.status.idle":"2022-05-07T15:18:48.796292Z","shell.execute_reply.started":"2022-05-07T15:18:45.872157Z","shell.execute_reply":"2022-05-07T15:18:48.795206Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_tokenized = train_all_tokenized.remove_columns(['id'])\ntest_tokenized = test_all_tokenized.remove_columns(['id'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T15:19:01.368942Z","iopub.execute_input":"2022-05-07T15:19:01.369258Z","iopub.status.idle":"2022-05-07T15:19:01.382649Z","shell.execute_reply.started":"2022-05-07T15:19:01.369226Z","shell.execute_reply":"2022-05-07T15:19:01.381416Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nBERT (Bidirectional Encoder Representations from Transformers) is a state of the art approach to various NLP tasks. It uses a Transformer architecture and relies heavily on the concept of pre-training.\n\nWe'll use a pre-trained BERT-base model, provided in huggingface [transformers](https://github.com/huggingface/transformers) repo. It gives us a pytorch implementation for the classic BERT architecture, as well as a tokenizer and weights pre-trained on a public English corpus (Wikipedia).\n\nPlease follow these [installation instrucitons](https://github.com/huggingface/transformers#installation) before proceeding.","metadata":{"id":"smey0wVLOG3Q"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_pretrained_model(model_name, num_labels):\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n    trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n    total_params = 0\n    trainable_params = 0\n\n    for p in model.parameters():\n        p.requires_grad = False\n        total_params += p.numel()\n\n    for layer in trainable_layers:\n        for p in layer.parameters():\n            p.requires_grad = True\n            trainable_params += p.numel()\n\n    print(f\"Total parameters count: {total_params}\") # ~108M\n    print(f\"Trainable parameters count: {trainable_params}\") # ~7M\n\n    return model","metadata":{"id":"XtuQ-VCYOJM_","execution":{"iopub.status.busy":"2022-05-07T15:19:03.886781Z","iopub.execute_input":"2022-05-07T15:19:03.887067Z","iopub.status.idle":"2022-05-07T15:19:03.895674Z","shell.execute_reply.started":"2022-05-07T15:19:03.887036Z","shell.execute_reply":"2022-05-07T15:19:03.894760Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"num_labels = 2\n# model_name = \"bert-base-uncased\"\nmodel_name = 'prajjwal1/bert-small'\nmodel = load_pretrained_model(model_name, num_labels)","metadata":{"id":"dfJ3Mun0L4uR","outputId":"5891754e-9c58-41a3-ec1a-64fddca917d1","execution":{"iopub.status.busy":"2022-05-07T17:20:25.046973Z","iopub.execute_input":"2022-05-07T17:20:25.047301Z","iopub.status.idle":"2022-05-07T17:20:26.364984Z","shell.execute_reply.started":"2022-05-07T17:20:25.047268Z","shell.execute_reply":"2022-05-07T17:20:26.364002Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"# Data loader","metadata":{"id":"-deCefEhMOe5"}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\n# needed for DP training\nMAX_PHYSICAL_BATCH_SIZE = 2","metadata":{"id":"ksXBCp_6Mwqy","execution":{"iopub.status.busy":"2022-05-07T15:19:22.551800Z","iopub.execute_input":"2022-05-07T15:19:22.552145Z","iopub.status.idle":"2022-05-07T15:19:22.557110Z","shell.execute_reply.started":"2022-05-07T15:19:22.552098Z","shell.execute_reply":"2022-05-07T15:19:22.555772Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_tokenized, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_tokenized, batch_size=BATCH_SIZE)","metadata":{"id":"8xxS086tdJd8","execution":{"iopub.status.busy":"2022-05-07T15:19:24.462783Z","iopub.execute_input":"2022-05-07T15:19:24.463216Z","iopub.status.idle":"2022-05-07T15:19:24.470233Z","shell.execute_reply.started":"2022-05-07T15:19:24.463168Z","shell.execute_reply":"2022-05-07T15:19:24.468901Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"RMuUO5vdN2Ux"}},{"cell_type":"markdown","source":"## Utils","metadata":{"id":"ZPvOGP6ZN9TD"}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nsigmoid = torch.nn.Sigmoid()\n\n# https://huggingface.co/docs/datasets/metrics\ndef calculate_result(labels, probs, threshold=0.5):\n    preds = np.where(probs >= threshold, 1, 0)\n    return {\n        'accuracy': np.round(accuracy_score(labels, preds), 4),\n        'f1': np.round(f1_score(labels, preds), 4),\n        'auc': np.round(roc_auc_score(labels, probs), 4)\n    }\n\ndef evaluate(model, test_dataloader, epoch):    \n    model.eval()\n\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n    progress_bar = tqdm(range(len(test_dataloader)), desc=f'Epoch {epoch} (Test)')\n    \n    for batch in test_dataloader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        \n        probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n        labels = inputs[target].detach().cpu().numpy()\n        \n        losses.append(loss.item())\n        total_probs = torch.cat((total_probs, probs), dim=0)\n        total_labels.extend(labels)\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(\n            loss=np.round(np.mean(losses), 4), \n            f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n        )\n    \n    model.train()\n    test_result = calculate_result(total_labels, total_probs)\n    return np.mean(losses), test_result, total_probs\n\ndef train(model, train_dataloader, epoch):\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n    progress_bar = tqdm(range(len(train_dataloader)), desc=f'Epoch {epoch} (Train)')\n\n    for step, data in enumerate(train_dataloader):\n        optimizer.zero_grad()\n\n        inputs = {k: v.to(device) for k, v in data.items()}\n        outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n\n        # targets = data[target].to(device, dtype = torch.long)\n        # loss = loss_function(outputs.logits, targets)\n        loss = outputs[0]\n\n        loss.backward()\n        optimizer.step()\n\n        losses.append(loss.item())\n\n        # preds = np.argmax(outputs.logits.detach().cpu().numpy(), axis=1)\n        probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n        labels = inputs[target].detach().cpu().numpy()\n        \n        total_probs = torch.cat((total_probs, probs), dim=0)\n        total_labels.extend(labels)\n\n        progress_bar.update(1)\n        progress_bar.set_postfix(\n            loss=np.round(np.mean(losses), 4), \n            f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n        )\n\n\n    train_loss = np.mean(losses)\n    train_result = calculate_result(np.array(total_labels), np.array(total_probs))\n\n    return train_loss, train_result, total_probs\n\ndef dump_results(epoch=None):\n    train_df = pd.DataFrame({'id':train_all_tokenized['id'], 'labels':train_all_tokenized[target], \n      'probs': train_probs, 'split':['train']* len(train_all_tokenized)\n    })\n    test_df = pd.DataFrame({'id':test_all_tokenized['id'], 'labels':test_all_tokenized[target], \n      'probs': test_probs, 'split':['test']* len(test_all_tokenized)\n    })\n\n    total_df = pd.concat([train_df, test_df],ignore_index=True)\n\n    if epoch is None:\n        total_df.to_csv('results.csv', index=False)\n    else:\n        total_df.to_csv(f'results_{epoch}.csv', index=False)\n        \n    # return preds_df","metadata":{"id":"YMVfXSNXNszY","execution":{"iopub.status.busy":"2022-05-07T17:09:44.100324Z","iopub.execute_input":"2022-05-07T17:09:44.100628Z","iopub.status.idle":"2022-05-07T17:09:44.128983Z","shell.execute_reply.started":"2022-05-07T17:09:44.100586Z","shell.execute_reply":"2022-05-07T17:09:44.127363Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# Define optimizer\nLEARNING_RATE = 1e-3\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)","metadata":{"id":"4p90SUEiOUEU","execution":{"iopub.status.busy":"2022-05-07T17:20:36.607643Z","iopub.execute_input":"2022-05-07T17:20:36.608229Z","iopub.status.idle":"2022-05-07T17:20:36.614910Z","shell.execute_reply.started":"2022-05-07T17:20:36.608194Z","shell.execute_reply":"2022-05-07T17:20:36.613835Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"## Loop","metadata":{"id":"LEmY9e-XN-yR"}},{"cell_type":"code","source":"gc.collect()\nEPOCHS = 2\n# Set the model to train mode (HuggingFace models load in eval mode)\nmodel = model.train().to(device)\n\nfor epoch in range(1, EPOCHS+1):\n    train_loss, train_result, train_probs = train(model, train_dataloader, epoch)\n    test_loss, test_result, test_probs = evaluate(model, test_dataloader, epoch)\n\n    print(\n      f\"Epoch: {epoch} | \"\n      f\"Train loss: {train_loss:.3f} | \"\n      f\"Train result: {train_result} |\\n\"\n      f\"Test loss: {test_loss:.3f} | \"\n      f\"Test result: {test_result} | \"\n    )\n    dump_results(epoch)","metadata":{"id":"ZbJF65L_OASJ","outputId":"a6bd0251-3eac-4e8f-d8c6-381ff3fa075b","execution":{"iopub.status.busy":"2022-05-07T17:20:42.667028Z","iopub.execute_input":"2022-05-07T17:20:42.667360Z","iopub.status.idle":"2022-05-07T18:23:17.054492Z","shell.execute_reply.started":"2022-05-07T17:20:42.667326Z","shell.execute_reply":"2022-05-07T18:23:17.053511Z"},"trusted":true},"execution_count":75,"outputs":[]}]}
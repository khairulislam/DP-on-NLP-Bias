{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is done following \n* [Building text classifier with Differential Privacy](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)\n* [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.4.0/custom_datasets.html#seq-imdb)","metadata":{"id":"huyLXVRDUwFw"}},{"cell_type":"markdown","source":"# Libraries\nhttps://huggingface.co/docs/transformers/training","metadata":{"id":"n3CQPh6pRJpP"}},{"cell_type":"markdown","source":"## Install","metadata":{"id":"tB5WsXAHyBZv"}},{"cell_type":"code","source":"!pip install opacus","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:22:46.206150Z","iopub.execute_input":"2022-05-08T21:22:46.206505Z","iopub.status.idle":"2022-05-08T21:22:57.729364Z","shell.execute_reply.started":"2022-05-08T21:22:46.206411Z","shell.execute_reply":"2022-05-08T21:22:57.728476Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:22:57.733079Z","iopub.execute_input":"2022-05-08T21:22:57.733328Z","iopub.status.idle":"2022-05-08T21:23:14.403477Z","shell.execute_reply.started":"2022-05-08T21:22:57.733302Z","shell.execute_reply":"2022-05-08T21:23:14.402741Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{"id":"nxhRYG1UyFLJ"}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom opacus.utils.batch_memory_manager import BatchMemoryManager\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\npd.set_option('display.max_columns', None)","metadata":{"id":"MjZvbCbYx-no","execution":{"iopub.status.busy":"2022-05-08T21:23:14.404760Z","iopub.execute_input":"2022-05-08T21:23:14.404986Z","iopub.status.idle":"2022-05-08T21:23:15.167550Z","shell.execute_reply.started":"2022-05-08T21:23:14.404956Z","shell.execute_reply":"2022-05-08T21:23:15.166750Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:23:15.169424Z","iopub.execute_input":"2022-05-08T21:23:15.169692Z","iopub.status.idle":"2022-05-08T21:23:15.185372Z","shell.execute_reply.started":"2022-05-08T21:23:15.169656Z","shell.execute_reply":"2022-05-08T21:23:15.184686Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef seed_torch(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\nglobal_seed = 2022\nseed_torch(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:23:15.186466Z","iopub.execute_input":"2022-05-08T21:23:15.186779Z","iopub.status.idle":"2022-05-08T21:23:15.195047Z","shell.execute_reply.started":"2022-05-08T21:23:15.186742Z","shell.execute_reply":"2022-05-08T21:23:15.194360Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Get device","metadata":{"id":"CSnXCK2k6KvV"}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"id":"N01plY896Mps","outputId":"3d0d33d9-0b02-4ad8-c8c0-b227b57bf1fb","execution":{"iopub.status.busy":"2022-05-08T21:23:15.196444Z","iopub.execute_input":"2022-05-08T21:23:15.196692Z","iopub.status.idle":"2022-05-08T21:23:15.252598Z","shell.execute_reply.started":"2022-05-08T21:23:15.196657Z","shell.execute_reply":"2022-05-08T21:23:15.251889Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load tokenized data\n\nFrom my [other notebook](https://www.kaggle.com/code/khairulislam/tokenize-jigsaw-comments). The dataset is tokenized from the [Jigsaw competition]( https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) and [all_data.csv](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv)","metadata":{}},{"cell_type":"code","source":"text = 'comment_text'\ntarget = 'labels'\nroot = '/kaggle/input/tokenize-jigsaw-comments/'","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:23:15.253772Z","iopub.execute_input":"2022-05-08T21:23:15.254020Z","iopub.status.idle":"2022-05-08T21:23:15.261323Z","shell.execute_reply.started":"2022-05-08T21:23:15.253984Z","shell.execute_reply":"2022-05-08T21:23:15.260419Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pickle\n    \nwith open(root + 'test.pkl', 'rb') as input_file:\n    test_all_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'train_undersampled.pkl', 'rb') as input_file:\n    train_all_tokenized = pickle.load(input_file)\n    input_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:23:15.264330Z","iopub.execute_input":"2022-05-08T21:23:15.264529Z","iopub.status.idle":"2022-05-08T21:23:17.831356Z","shell.execute_reply.started":"2022-05-08T21:23:15.264501Z","shell.execute_reply":"2022-05-08T21:23:17.830408Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# train_tokenized_small = train_all_tokenized.shuffle(seed=global_seed).select(range(1000))\n# test_tokenized_small = test_all_tokenized.shuffle(seed=global_seed).select(range(100))\n\n# train_tokenized = train_tokenized_small.remove_columns(['id'])\n# test_tokenized = test_tokenized_small.remove_columns(['id'])\n\n# only keep int/float columns\ntrain_tokenized = train_all_tokenized.remove_columns(['id'])\ntest_tokenized = test_all_tokenized.remove_columns(['id'])","metadata":{"execution":{"iopub.status.busy":"2022-05-08T21:23:17.833149Z","iopub.execute_input":"2022-05-08T21:23:17.833460Z","iopub.status.idle":"2022-05-08T21:23:17.845287Z","shell.execute_reply.started":"2022-05-08T21:23:17.833422Z","shell.execute_reply":"2022-05-08T21:23:17.844502Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nBERT (Bidirectional Encoder Representations from Transformers) is a state of the art approach to various NLP tasks. It uses a Transformer architecture and relies heavily on the concept of pre-training.\n\nWe'll use a pre-trained BERT-base model, provided in huggingface [transformers](https://github.com/huggingface/transformers) repo. It gives us a pytorch implementation for the classic BERT architecture, as well as a tokenizer and weights pre-trained on a public English corpus (Wikipedia).\n\nPlease follow these [installation instrucitons](https://github.com/huggingface/transformers#installation) before proceeding.","metadata":{"id":"smey0wVLOG3Q"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_pretrained_model(model_name, num_labels):\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n    trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n    total_params = 0\n    trainable_params = 0\n\n    for p in model.parameters():\n        p.requires_grad = False\n        total_params += p.numel()\n\n    for layer in trainable_layers:\n        for p in layer.parameters():\n            p.requires_grad = True\n            trainable_params += p.numel()\n\n    print(f\"Total parameters count: {total_params}\") # ~108M\n    print(f\"Trainable parameters count: {trainable_params}\") # ~7M\n\n    return model","metadata":{"id":"XtuQ-VCYOJM_","execution":{"iopub.status.busy":"2022-05-08T21:23:17.848791Z","iopub.execute_input":"2022-05-08T21:23:17.849232Z","iopub.status.idle":"2022-05-08T21:23:17.857661Z","shell.execute_reply.started":"2022-05-08T21:23:17.849179Z","shell.execute_reply":"2022-05-08T21:23:17.856785Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"num_labels = 2\n# model_name = \"bert-base-uncased\"\nmodel_name = 'prajjwal1/bert-small' # https://huggingface.co/prajjwal1/bert-small","metadata":{"id":"dfJ3Mun0L4uR","outputId":"5891754e-9c58-41a3-ec1a-64fddca917d1","execution":{"iopub.status.busy":"2022-05-08T21:23:17.859242Z","iopub.execute_input":"2022-05-08T21:23:17.860274Z","iopub.status.idle":"2022-05-08T21:23:17.868513Z","shell.execute_reply.started":"2022-05-08T21:23:17.860238Z","shell.execute_reply":"2022-05-08T21:23:17.867486Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Private Training","metadata":{"id":"Cf0mMuDWNQla"}},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nsigmoid = torch.nn.Sigmoid()\n\n# https://huggingface.co/docs/datasets/metrics\ndef calculate_result(labels, probs, threshold=0.5):\n    preds = np.where(probs >= threshold, 1, 0)\n    return {\n        'accuracy': np.round(accuracy_score(labels, preds), 4),\n        'f1': np.round(f1_score(labels, preds), 4),\n        'auc': np.round(roc_auc_score(labels, probs), 4)\n    }\n\ndef evaluate(model, test_dataloader, epoch):    \n    model.eval()\n\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n    progress_bar = tqdm(range(len(test_dataloader)), desc=f'Epoch {epoch} (Test)')\n    \n    for batch in test_dataloader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        \n        probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n        labels = inputs[target].detach().cpu().numpy()\n        \n        losses.append(loss.item())\n        total_probs = torch.cat((total_probs, probs), dim=0)\n        total_labels.extend(labels)\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(\n            loss=np.round(np.mean(losses), 4), \n            f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n        )\n    \n    model.train()\n    test_result = calculate_result(total_labels, total_probs)\n    return np.mean(losses), test_result, total_probs\n\ndef dp_train(model, train_dataloader, epoch):\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n\n    with BatchMemoryManager(\n        data_loader=train_dataloader, \n        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n        optimizer=optimizer\n    ) as memory_safe_data_loader:\n        progress_bar = tqdm(range(len(memory_safe_data_loader)), desc=f'Epoch {epoch} (Train)')\n\n        for step, data in enumerate(memory_safe_data_loader):\n            optimizer.zero_grad()\n\n            inputs = {k: v.to(device) for k, v in data.items()}\n            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n\n            # loss = loss_function(outputs.logits, targets)\n            loss = outputs[0]\n\n            loss.backward()\n            optimizer.step()\n\n            losses.append(loss.item())\n\n            # preds = np.argmax(outputs.logits.detach().cpu().numpy(), axis=1)\n            probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n            labels = inputs[target].detach().cpu().numpy()\n            \n            total_probs = torch.cat((total_probs, probs), dim=0)\n            total_labels.extend(labels)\n\n            progress_bar.update(1)\n            progress_bar.set_postfix(\n                loss=np.round(np.mean(losses), 4), \n                f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n            )\n\n    train_loss = np.mean(losses)\n    train_result = calculate_result(np.array(total_labels), np.array(total_probs))\n\n    return train_loss, train_result, total_probs\n\ndef dump_results(epoch=None):\n    n_train, n_test = len(train_probs), len(test_probs)\n    train_df = pd.DataFrame({\n        'id':train_all_tokenized['id'][:n_train], 'labels':train_all_tokenized[target][:n_train], \n        'probs': train_probs, 'split':['train']* n_train\n    })\n    test_df = pd.DataFrame({\n        'id':test_all_tokenized['id'][:n_test], 'labels':test_all_tokenized[target][:n_test], \n        'probs': test_probs, 'split':['test']* n_test\n    })\n\n    total_df = pd.concat([train_df, test_df],ignore_index=True)\n\n    if epoch is None:\n        total_df.to_csv('results_dp.csv', index=False)\n    else:\n        total_df.to_csv(f'results_dp_{epoch}.csv', index=False)","metadata":{"id":"ae5nsaxPy6LC","execution":{"iopub.status.busy":"2022-05-08T21:23:17.870099Z","iopub.execute_input":"2022-05-08T21:23:17.870451Z","iopub.status.idle":"2022-05-08T21:23:18.200785Z","shell.execute_reply.started":"2022-05-08T21:23:17.870371Z","shell.execute_reply":"2022-05-08T21:23:18.199923Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Data loader\n\n[How to choose batch size in DP](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\n\n# needed for DP training\nMAX_PHYSICAL_BATCH_SIZE = 64\n\ntrain_dataloader = DataLoader(train_tokenized, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_tokenized, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T01:06:00.033627Z","iopub.execute_input":"2022-05-09T01:06:00.033887Z","iopub.status.idle":"2022-05-09T01:06:00.040418Z","shell.execute_reply.started":"2022-05-09T01:06:00.033858Z","shell.execute_reply":"2022-05-09T01:06:00.037975Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Model and optimizer","metadata":{}},{"cell_type":"code","source":"EPOCHS = 1\ndelta_list = [5e-2, 1e-3, 1e-5]\nNOISE_MULTIPLIER = 0.3\nLEARNING_RATE = 1e-3\nMAX_GRAD_NORM = 1","metadata":{"id":"J55PhL-SNXIy","execution":{"iopub.status.busy":"2022-05-09T01:06:17.861070Z","iopub.execute_input":"2022-05-09T01:06:17.863231Z","iopub.status.idle":"2022-05-09T01:06:17.868081Z","shell.execute_reply.started":"2022-05-09T01:06:17.863197Z","shell.execute_reply":"2022-05-09T01:06:17.867294Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# load a fresh model each time\nmodel = load_pretrained_model(model_name, num_labels)\n\n# Set the model to train mode (HuggingFace models load in eval mode)\nmodel = model.train().to(device)\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n\n# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)","metadata":{"id":"2VJx_xjQNTaZ","execution":{"iopub.status.busy":"2022-05-09T01:06:12.438033Z","iopub.execute_input":"2022-05-09T01:06:12.438569Z","iopub.status.idle":"2022-05-09T01:06:13.731983Z","shell.execute_reply.started":"2022-05-09T01:06:12.438534Z","shell.execute_reply":"2022-05-09T01:06:13.731289Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Privacy Engine","metadata":{"id":"b7EFOaY2NeiR"}},{"cell_type":"code","source":"from opacus import PrivacyEngine\n\nprivacy_engine = PrivacyEngine()","metadata":{"id":"R00MPKNsh205","execution":{"iopub.status.busy":"2022-05-08T23:58:22.864383Z","iopub.execute_input":"2022-05-08T23:58:22.864649Z","iopub.status.idle":"2022-05-08T23:58:22.868779Z","shell.execute_reply.started":"2022-05-08T23:58:22.864621Z","shell.execute_reply":"2022-05-08T23:58:22.867955Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# using this method decreases how many batches BatchMemoryManager can load\n# e.g. with 1000 examples, 128 batch size, 64 max physical batch size, using BatchMemoryManager \n# with train_dataloader before applying this function returns length 16. But applying this method \n# on, same train_dataloader makes a BatchMemoryManager of length 15 instead.\n# EPSILON = 10\n# # DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarant\n# DELTA = 5e-2\n\n# model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=train_dataloader,\n#     target_delta=DELTA,\n#     target_epsilon=EPSILON, \n#     epochs=EPOCHS,\n#     max_grad_norm=MAX_GRAD_NORM,\n# )\n\nmodel, optimizer, train_dataloader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=train_dataloader,\n    noise_multiplier=NOISE_MULTIPLIER,\n    max_grad_norm=MAX_GRAD_NORM,\n    poisson_sampling=False,\n)","metadata":{"id":"Wta4qx6mNgqB","execution":{"iopub.status.busy":"2022-05-09T01:06:20.990640Z","iopub.execute_input":"2022-05-09T01:06:20.990892Z","iopub.status.idle":"2022-05-09T01:06:21.005987Z","shell.execute_reply.started":"2022-05-09T01:06:20.990864Z","shell.execute_reply":"2022-05-09T01:06:21.005067Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"## Loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(1, EPOCHS+1):\n    gc.collect()\n    \n    train_loss, train_result, train_probs = dp_train(model, train_dataloader, epoch)\n    test_loss, test_result, test_probs = evaluate(model, test_dataloader, epoch)\n    \n    epsilons = []\n    for delta in delta_list:\n        epsilons.append(privacy_engine.get_epsilon(delta))\n\n    print(\n      f\"Epoch: {epoch} | \"\n      f\"ɛ: {np.round(epsilons, 2)} |\"\n      f\"Train loss: {train_loss:.3f} | \"\n      f\"Train result: {train_result} |\\n\"\n      f\"Test loss: {test_loss:.3f} | \"\n      f\"Test result: {test_result} | \"\n    )\n    \n    dump_results(epoch)\n    lr_scheduler.step()","metadata":{"id":"RkZPY4iIUUOw","outputId":"aebf8df4-0009-4678-b0d5-392753e178e9","execution":{"iopub.status.busy":"2022-05-09T01:06:24.147912Z","iopub.execute_input":"2022-05-09T01:06:24.148490Z","iopub.status.idle":"2022-05-09T01:27:08.348560Z","shell.execute_reply.started":"2022-05-09T01:06:24.148452Z","shell.execute_reply":"2022-05-09T01:27:08.347807Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"Epoch 1 (Train): 100%\n4511/4512 [19:55<00:00, 3.38it/s, f1=0.799, loss=0.8]\nEpoch 1 (Test): 100%\n1521/1521 [04:47<00:00, 4.21it/s, f1=0.488, loss=0.67]\nEpoch: 1 | ɛ: [ 51.14  80.88 103.91] |Train loss: 0.800 | Train result: {'accuracy': 0.8026, 'f1': 0.7993, 'auc': 0.8819} |\nTest loss: 0.670 | Test result: {'accuracy': 0.867, 'f1': 0.4879, 'auc': 0.9102} | ","metadata":{}},{"cell_type":"markdown","source":"|Model|Train Time|Learning rate|batch size|epoch |          |  train |         |       | test    |       |Noise multiplier| Epsilon ([5e-2, 1e-3, 1e-5]) |\n|-----|----------|-------------|----------|------|----------|--------|---------|-------|---------|-------|----------------|------------------------------|\n|     |          |             |          |      |  loss    |  f1    |  auc    | loss  | f1      | auc   |                |                              |\n|bert small with lr scheduler|27 min|1e-3|64(32)|1| 0.793   | 0.8183 |  0.8776 | 1.016 |  0.3964 | 0.9017|     0.1        |   1058.57, 1097.69, 1143.74  |\n|     | 25min    |      1e-3   |          |   2  |  0.766   | 0.8297 |  0.8741 | 0.794 |  0.4338 | 0.8961|     0.1        |   2090.53, 2129.65, 2175.7   |\n|bert small with lr scheduler|17 min|5e-4|32(32)|1| 0.774   | 0.815  |  0.8937 | 0.774 |  0.4765 | 0.9222|     0.1        |   1144.95, 1184.07, 1230.12  |\n|     | 21min    |      5e-4   |          |   2  |  0.736   | 0.8375 |  0.9067 | 0.736 |  0.4739 | 0.9259|     0.1        |   2237.59, 2276.71, 2322.77  |\n|     | 17min    |      5e-4   |          |   3  |  0.737   | 0.8325 |  0.8976 | 0.788 |  0.4352 | 0.9174|     0.1        |   3330.24, 3369.36, 3415.41  |\n|bert small with lr scheduler|21 min|1e-3|128(64)|1| 0.774  | 0.8204 |  0.8937 | 0.774 |  0.4765 | 0.9222|     0.1        |   1144.95, 1184.07, 1230.12  |\n|     |          |      1e-3   |          |   2  |  0.736   | 0.8375 |  0.9067 | 0.736 |  0.4739 | 0.9259|     0.1        |   2237.59, 2276.71, 2322.77  |\n|     |          |      1e-3   |          |   3  |  0.737   | 0.8325 |  0.8976 | 0.788 |  0.4352 | 0.9174|     0.1        |   3330.24, 3369.36, 3415.41  |\n|bert small with lr scheduler|20 min|1e-3|128(64)|1| 0.779  | 0.8086 |  0.8885 | 0.689 |  0.4790 | 0.9161|     0.2        |     28.38, 47.94, 70.96      |\n|     | 21min    |      1e-3   |          |   2  |  0.758   | 0.8282 |  0.8999 | 0.689 |  0.4750 | 0.9179|     0.2        |     42.57, 64.04, 87.06      |\n|     | 17min    |      1e-3   |          |   3  |  0.746   | 0.8318 |  0.8984 | 0.696 |  0.4733 | 0.9175|     0.2        |     50.56, 80.14, 103.16     |\n|     | 20 min   |      1e-3   |  128(64) |   1  |  0.844   | 0.8006 |  0.8721 | 0.722 |  0.4650 | 0.9006|     0.25       |     12.38, 23.28, 34.79      |\n|     |          |      1e-3   |          |   2  |  0.797   | 0.8236 |  0.8831 | 0.707 |  0.4726 | 0.9026|     0.25       |     17.11, 30.15, 42.89      |\n|     |          |      1e-3   |          |   3  |  0.783   | 0.8237 |  0.8760 | 0.777 |  0.4749 | 0.8934|     0.25       |     21.32, 34.89, 50.24      |\n|     | 22min    |      1e-3   |  128(64) |   1  |  0.794   | 0.7977 |  0.8814 | 0.698 |  0.4713 | 0.9025|     0.3        |     51.14, 80.88, 103.91     |\n|     |          |      1e-3   |          |   2  |  0.773   | 0.8229 |  0.8894 | 0.818 |  0.4455 | 0.9055|     0.3        |     51.72, 81.63, 104.65     |\n|     |          |      1e-3   |          |   3  |  0.770   | 0.8269 |  0.8864 | 0.801 |  0.4445 | 0.9018|     0.3        |     52.30, 82.37, 105.40     |","metadata":{}},{"cell_type":"markdown","source":"## Save model\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html","metadata":{}},{"cell_type":"code","source":"model_path = 'dp_model_state_dict.pt'\ntorch.save(model.state_dict(), model_path)\n\n# model.load_state_dict(torch.load(model_path))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-08T23:56:03.023612Z","iopub.execute_input":"2022-05-08T23:56:03.023868Z","iopub.status.idle":"2022-05-08T23:56:03.262610Z","shell.execute_reply.started":"2022-05-08T23:56:03.023839Z","shell.execute_reply":"2022-05-08T23:56:03.261884Z"},"trusted":true},"execution_count":35,"outputs":[]}]}
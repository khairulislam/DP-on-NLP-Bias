{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is done following \n* [Building text classifier with Differential Privacy](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)\n* [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.4.0/custom_datasets.html#seq-imdb)","metadata":{"id":"huyLXVRDUwFw"}},{"cell_type":"markdown","source":"# Libraries\nhttps://huggingface.co/docs/transformers/training","metadata":{"id":"n3CQPh6pRJpP"}},{"cell_type":"markdown","source":"## Install","metadata":{"id":"tB5WsXAHyBZv"}},{"cell_type":"code","source":"!pip install opacus","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:27:27.069842Z","iopub.execute_input":"2022-05-07T18:27:27.070161Z","iopub.status.idle":"2022-05-07T18:27:39.403100Z","shell.execute_reply.started":"2022-05-07T18:27:27.070065Z","shell.execute_reply":"2022-05-07T18:27:39.402284Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:27:39.406203Z","iopub.execute_input":"2022-05-07T18:27:39.406418Z","iopub.status.idle":"2022-05-07T18:27:57.363511Z","shell.execute_reply.started":"2022-05-07T18:27:39.406387Z","shell.execute_reply":"2022-05-07T18:27:57.362763Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{"id":"nxhRYG1UyFLJ"}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom opacus.utils.batch_memory_manager import BatchMemoryManager\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\npd.set_option('display.max_columns', None)","metadata":{"id":"MjZvbCbYx-no","execution":{"iopub.status.busy":"2022-05-07T18:27:57.364862Z","iopub.execute_input":"2022-05-07T18:27:57.365108Z","iopub.status.idle":"2022-05-07T18:27:58.080678Z","shell.execute_reply.started":"2022-05-07T18:27:57.365075Z","shell.execute_reply":"2022-05-07T18:27:58.079911Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:28:00.026956Z","iopub.execute_input":"2022-05-07T18:28:00.027818Z","iopub.status.idle":"2022-05-07T18:28:00.040375Z","shell.execute_reply.started":"2022-05-07T18:28:00.027765Z","shell.execute_reply":"2022-05-07T18:28:00.039426Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef seed_torch(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\nglobal_seed = 2022\nseed_torch(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:28:02.233741Z","iopub.execute_input":"2022-05-07T18:28:02.234538Z","iopub.status.idle":"2022-05-07T18:28:02.243171Z","shell.execute_reply.started":"2022-05-07T18:28:02.234479Z","shell.execute_reply":"2022-05-07T18:28:02.242413Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Get device","metadata":{"id":"CSnXCK2k6KvV"}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"id":"N01plY896Mps","outputId":"3d0d33d9-0b02-4ad8-c8c0-b227b57bf1fb","execution":{"iopub.status.busy":"2022-05-07T18:28:04.505181Z","iopub.execute_input":"2022-05-07T18:28:04.505770Z","iopub.status.idle":"2022-05-07T18:28:04.563774Z","shell.execute_reply.started":"2022-05-07T18:28:04.505730Z","shell.execute_reply":"2022-05-07T18:28:04.562826Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load tokenized data\n\nFrom my [other notebook](https://www.kaggle.com/code/khairulislam/tokenize-jigsaw-comments). The dataset is tokenized from the [Jigsaw competition]( https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) and [all_data.csv](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv)","metadata":{}},{"cell_type":"code","source":"text = 'comment_text'\ntarget = 'labels'\nroot = '/kaggle/input/tokenize-jigsaw-comments/'","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:28:06.777998Z","iopub.execute_input":"2022-05-07T18:28:06.780688Z","iopub.status.idle":"2022-05-07T18:28:06.786735Z","shell.execute_reply.started":"2022-05-07T18:28:06.780615Z","shell.execute_reply":"2022-05-07T18:28:06.786008Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pickle\n    \nwith open(root + 'test.pkl', 'rb') as input_file:\n    test_all_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'train_undersampled.pkl', 'rb') as input_file:\n    train_all_tokenized = pickle.load(input_file)\n    input_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:28:09.168143Z","iopub.execute_input":"2022-05-07T18:28:09.168623Z","iopub.status.idle":"2022-05-07T18:28:11.453840Z","shell.execute_reply.started":"2022-05-07T18:28:09.168589Z","shell.execute_reply":"2022-05-07T18:28:11.453103Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# train_tokenized_small = train_all_tokenized.shuffle(seed=global_seed).select(range(100))\n# test_tokenized_small = test_all_tokenized.shuffle(seed=global_seed).select(range(100))\n\n# train_tokenized = train_tokenized_small.remove_columns(['id'])\n# test_tokenized = test_tokenized_small.remove_columns(['id'])\n\n# only keep int/float columns\ntrain_tokenized = train_all_tokenized.remove_columns(['id'])\ntest_tokenized = test_all_tokenized.remove_columns(['id'])","metadata":{"execution":{"iopub.status.busy":"2022-05-07T18:28:13.417644Z","iopub.execute_input":"2022-05-07T18:28:13.418117Z","iopub.status.idle":"2022-05-07T18:28:13.429308Z","shell.execute_reply.started":"2022-05-07T18:28:13.418079Z","shell.execute_reply":"2022-05-07T18:28:13.428557Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nBERT (Bidirectional Encoder Representations from Transformers) is a state of the art approach to various NLP tasks. It uses a Transformer architecture and relies heavily on the concept of pre-training.\n\nWe'll use a pre-trained BERT-base model, provided in huggingface [transformers](https://github.com/huggingface/transformers) repo. It gives us a pytorch implementation for the classic BERT architecture, as well as a tokenizer and weights pre-trained on a public English corpus (Wikipedia).\n\nPlease follow these [installation instrucitons](https://github.com/huggingface/transformers#installation) before proceeding.","metadata":{"id":"smey0wVLOG3Q"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_pretrained_model(model_name, num_labels):\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n    trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n    total_params = 0\n    trainable_params = 0\n\n    for p in model.parameters():\n        p.requires_grad = False\n        total_params += p.numel()\n\n    for layer in trainable_layers:\n        for p in layer.parameters():\n            p.requires_grad = True\n            trainable_params += p.numel()\n\n    print(f\"Total parameters count: {total_params}\") # ~108M\n    print(f\"Trainable parameters count: {trainable_params}\") # ~7M\n\n    return model","metadata":{"id":"XtuQ-VCYOJM_","execution":{"iopub.status.busy":"2022-05-07T18:28:16.762134Z","iopub.execute_input":"2022-05-07T18:28:16.762391Z","iopub.status.idle":"2022-05-07T18:28:16.770240Z","shell.execute_reply.started":"2022-05-07T18:28:16.762362Z","shell.execute_reply":"2022-05-07T18:28:16.769413Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"num_labels = 2\n# model_name = \"bert-base-uncased\"\nmodel_name = 'prajjwal1/bert-small'","metadata":{"id":"dfJ3Mun0L4uR","outputId":"5891754e-9c58-41a3-ec1a-64fddca917d1","execution":{"iopub.status.busy":"2022-05-07T18:28:18.809417Z","iopub.execute_input":"2022-05-07T18:28:18.810057Z","iopub.status.idle":"2022-05-07T18:28:18.813909Z","shell.execute_reply.started":"2022-05-07T18:28:18.810013Z","shell.execute_reply":"2022-05-07T18:28:18.813000Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Data loader","metadata":{"id":"-deCefEhMOe5"}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\n# needed for DP training\nMAX_PHYSICAL_BATCH_SIZE = 16","metadata":{"id":"ksXBCp_6Mwqy","execution":{"iopub.status.busy":"2022-05-07T19:47:57.715248Z","iopub.execute_input":"2022-05-07T19:47:57.715499Z","iopub.status.idle":"2022-05-07T19:47:57.719184Z","shell.execute_reply.started":"2022-05-07T19:47:57.715471Z","shell.execute_reply":"2022-05-07T19:47:57.718349Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_tokenized, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_tokenized, batch_size=BATCH_SIZE)","metadata":{"id":"8xxS086tdJd8","execution":{"iopub.status.busy":"2022-05-07T19:48:00.946480Z","iopub.execute_input":"2022-05-07T19:48:00.947029Z","iopub.status.idle":"2022-05-07T19:48:00.952436Z","shell.execute_reply.started":"2022-05-07T19:48:00.946994Z","shell.execute_reply":"2022-05-07T19:48:00.950359Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Private Training","metadata":{"id":"Cf0mMuDWNQla"}},{"cell_type":"code","source":"EPOCHS = 1\nEPSILON = 7.5\n# DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarant\nDELTA = 1e-5\ndelta_list = [5e-2, 1e-3, 1e-5]\nNOISE_MULTIPLIER = 0.1\nLEARNING_RATE = 1e-5\nMAX_GRAD_NORM = 1","metadata":{"id":"J55PhL-SNXIy","execution":{"iopub.status.busy":"2022-05-07T19:48:02.650846Z","iopub.execute_input":"2022-05-07T19:48:02.651703Z","iopub.status.idle":"2022-05-07T19:48:02.660145Z","shell.execute_reply.started":"2022-05-07T19:48:02.651656Z","shell.execute_reply":"2022-05-07T19:48:02.658508Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# load a fresh model each time\nmodel = load_pretrained_model(model_name, num_labels)\n\n# Set the model to train mode (HuggingFace models load in eval mode)\nmodel = model.train().to(device)\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)","metadata":{"id":"2VJx_xjQNTaZ","execution":{"iopub.status.busy":"2022-05-07T19:48:04.888268Z","iopub.execute_input":"2022-05-07T19:48:04.888540Z","iopub.status.idle":"2022-05-07T19:48:07.320643Z","shell.execute_reply.started":"2022-05-07T19:48:04.888497Z","shell.execute_reply":"2022-05-07T19:48:07.319922Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Privacy Engine","metadata":{"id":"b7EFOaY2NeiR"}},{"cell_type":"code","source":"from opacus import PrivacyEngine\n\nprivacy_engine = PrivacyEngine()","metadata":{"id":"R00MPKNsh205","execution":{"iopub.status.busy":"2022-05-07T18:29:12.011163Z","iopub.execute_input":"2022-05-07T18:29:12.011417Z","iopub.status.idle":"2022-05-07T18:29:12.015302Z","shell.execute_reply.started":"2022-05-07T18:29:12.011382Z","shell.execute_reply":"2022-05-07T18:29:12.014562Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=train_dataloader,\n#     target_delta=DELTA,\n#     target_epsilon=EPSILON, \n#     epochs=EPOCHS,\n#     max_grad_norm=MAX_GRAD_NORM,\n# )\n\nmodel, optimizer, train_dataloader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=train_dataloader,\n    noise_multiplier=NOISE_MULTIPLIER,\n    max_grad_norm=MAX_GRAD_NORM,\n    poisson_sampling=False,\n)","metadata":{"id":"Wta4qx6mNgqB","execution":{"iopub.status.busy":"2022-05-07T19:48:09.786940Z","iopub.execute_input":"2022-05-07T19:48:09.787200Z","iopub.status.idle":"2022-05-07T19:48:09.795447Z","shell.execute_reply.started":"2022-05-07T19:48:09.787170Z","shell.execute_reply":"2022-05-07T19:48:09.794704Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{"id":"weAw0ypQUedf"}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nsigmoid = torch.nn.Sigmoid()\n\n# https://huggingface.co/docs/datasets/metrics\ndef calculate_result(labels, probs, threshold=0.5):\n    preds = np.where(probs >= threshold, 1, 0)\n    return {\n        'accuracy': np.round(accuracy_score(labels, preds), 4),\n        'f1': np.round(f1_score(labels, preds), 4),\n        'auc': np.round(roc_auc_score(labels, probs), 4)\n    }\n\ndef evaluate(model, test_dataloader, epoch):    \n    model.eval()\n\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n    progress_bar = tqdm(range(len(test_dataloader)), desc=f'Epoch {epoch} (Test)')\n    \n    for batch in test_dataloader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        \n        probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n        labels = inputs[target].detach().cpu().numpy()\n        \n        losses.append(loss.item())\n        total_probs = torch.cat((total_probs, probs), dim=0)\n        total_labels.extend(labels)\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(\n            loss=np.round(np.mean(losses), 4), \n            f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n        )\n    \n    model.train()\n    test_result = calculate_result(total_labels, total_probs)\n    return np.mean(losses), test_result, total_probs\n\ndef dp_train(model, train_dataloader, epoch):\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n\n    with BatchMemoryManager(\n        data_loader=train_dataloader, \n        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n        optimizer=optimizer\n    ) as memory_safe_data_loader:\n        progress_bar = tqdm(range(len(memory_safe_data_loader)), desc=f'Epoch {epoch}')\n\n        for step, data in enumerate(memory_safe_data_loader):\n            optimizer.zero_grad()\n\n            inputs = {k: v.to(device) for k, v in data.items()}\n            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n\n            # loss = loss_function(outputs.logits, targets)\n            loss = outputs[0]\n\n            loss.backward()\n            optimizer.step()\n\n            losses.append(loss.item())\n\n            # preds = np.argmax(outputs.logits.detach().cpu().numpy(), axis=1)\n            probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n            labels = inputs[target].detach().cpu().numpy()\n            \n            total_probs = torch.cat((total_probs, probs), dim=0)\n            total_labels.extend(labels)\n\n            progress_bar.update(1)\n            progress_bar.set_postfix(\n                loss=np.round(np.mean(losses), 4), \n                f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n            )\n\n    train_loss = np.mean(losses)\n    train_result = calculate_result(np.array(total_labels), np.array(total_probs))\n\n    return train_loss, train_result, total_probs\n\ndef dump_results(epoch=None):\n    train_df = pd.DataFrame({'id':train_all_tokenized['id'], 'labels':train_all_tokenized[target], \n      'probs': train_probs, 'split':['train']* len(train_all_tokenized)\n    })\n    test_df = pd.DataFrame({'id':test_all_tokenized['id'], 'labels':test_all_tokenized[target], \n      'probs': test_probs, 'split':['test']* len(test_all_tokenized)\n    })\n\n    total_df = pd.concat([train_df, test_df],ignore_index=True)\n\n    if epoch is None:\n        total_df.to_csv('results_dp.csv', index=False)\n    else:\n        total_df.to_csv(f'results_dp_{epoch}.csv', index=False)","metadata":{"id":"ae5nsaxPy6LC","execution":{"iopub.status.busy":"2022-05-07T18:31:09.723971Z","iopub.execute_input":"2022-05-07T18:31:09.724239Z","iopub.status.idle":"2022-05-07T18:31:09.745824Z","shell.execute_reply.started":"2022-05-07T18:31:09.724209Z","shell.execute_reply":"2022-05-07T18:31:09.745011Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(1, EPOCHS+1):\n    gc.collect()\n    \n    train_loss, train_result, train_probs = dp_train(model, train_dataloader, epoch)\n    test_loss, test_result, test_probs = evaluate(model, test_dataloader, epoch)\n    \n    epsilons = []\n    for delta in delta_list:\n        epsilons.append(privacy_engine.get_epsilon(delta))\n\n    print(\n      f\"Epoch: {epoch} | \"\n      f\"ɛ: {np.round(epsilons, 2)} |\"\n      f\"Train loss: {train_loss:.3f} | \"\n      f\"Train result: {train_result} |\\n\"\n      f\"Eval loss: {test_loss:.3f} | \"\n      f\"Eval result: {test_result} | \"\n    )\n    \n    dump_results(epoch)","metadata":{"id":"RkZPY4iIUUOw","outputId":"aebf8df4-0009-4678-b0d5-392753e178e9","execution":{"iopub.status.busy":"2022-05-07T19:48:15.317073Z","iopub.execute_input":"2022-05-07T19:48:15.317339Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
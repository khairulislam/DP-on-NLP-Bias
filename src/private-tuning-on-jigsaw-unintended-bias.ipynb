{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is done following \n* [Building text classifier with Differential Privacy](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)\n* [Fine-tuning with custom datasets](https://huggingface.co/transformers/v3.4.0/custom_datasets.html#seq-imdb)","metadata":{"id":"huyLXVRDUwFw"}},{"cell_type":"markdown","source":"# Libraries\nhttps://huggingface.co/docs/transformers/training","metadata":{"id":"n3CQPh6pRJpP"}},{"cell_type":"markdown","source":"## Install","metadata":{"id":"tB5WsXAHyBZv"}},{"cell_type":"code","source":"!pip install opacus","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:22:41.301058Z","iopub.execute_input":"2022-05-09T13:22:41.301389Z","iopub.status.idle":"2022-05-09T13:22:52.314173Z","shell.execute_reply.started":"2022-05-09T13:22:41.301287Z","shell.execute_reply":"2022-05-09T13:22:52.313359Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install datasets\nimport datasets","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:22:52.316368Z","iopub.execute_input":"2022-05-09T13:22:52.316636Z","iopub.status.idle":"2022-05-09T13:23:09.318709Z","shell.execute_reply.started":"2022-05-09T13:22:52.316597Z","shell.execute_reply":"2022-05-09T13:23:09.317964Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Import","metadata":{"id":"nxhRYG1UyFLJ"}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom opacus.utils.batch_memory_manager import BatchMemoryManager\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport gc\n\npd.set_option('display.max_columns', None)","metadata":{"id":"MjZvbCbYx-no","execution":{"iopub.status.busy":"2022-05-09T13:23:09.320139Z","iopub.execute_input":"2022-05-09T13:23:09.320386Z","iopub.status.idle":"2022-05-09T13:23:10.216872Z","shell.execute_reply.started":"2022-05-09T13:23:09.320352Z","shell.execute_reply":"2022-05-09T13:23:10.216028Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:23:10.222470Z","iopub.execute_input":"2022-05-09T13:23:10.224913Z","iopub.status.idle":"2022-05-09T13:23:10.249634Z","shell.execute_reply.started":"2022-05-09T13:23:10.224870Z","shell.execute_reply":"2022-05-09T13:23:10.248997Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import random\n\ndef seed_torch(seed=7):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\nglobal_seed = 2022\nseed_torch(global_seed)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:23:10.253222Z","iopub.execute_input":"2022-05-09T13:23:10.253634Z","iopub.status.idle":"2022-05-09T13:23:10.266655Z","shell.execute_reply.started":"2022-05-09T13:23:10.253597Z","shell.execute_reply":"2022-05-09T13:23:10.265689Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Get device","metadata":{"id":"CSnXCK2k6KvV"}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)","metadata":{"id":"N01plY896Mps","outputId":"3d0d33d9-0b02-4ad8-c8c0-b227b57bf1fb","execution":{"iopub.status.busy":"2022-05-09T13:23:10.271147Z","iopub.execute_input":"2022-05-09T13:23:10.271829Z","iopub.status.idle":"2022-05-09T13:23:10.345123Z","shell.execute_reply.started":"2022-05-09T13:23:10.271789Z","shell.execute_reply":"2022-05-09T13:23:10.344231Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Load tokenized data\n\nFrom my [other notebook](https://www.kaggle.com/code/khairulislam/tokenize-jigsaw-comments). The dataset is tokenized from the [Jigsaw competition]( https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) and [all_data.csv](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/data?select=all_data.csv)","metadata":{}},{"cell_type":"code","source":"text = 'comment_text'\ntarget = 'labels'\nroot = '/kaggle/input/tokenize-jigsaw-comments/'","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:23:10.346815Z","iopub.execute_input":"2022-05-09T13:23:10.352484Z","iopub.status.idle":"2022-05-09T13:23:10.356361Z","shell.execute_reply.started":"2022-05-09T13:23:10.352443Z","shell.execute_reply":"2022-05-09T13:23:10.355504Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import pickle\n    \nwith open(root + 'test.pkl', 'rb') as input_file:\n    test_all_tokenized = pickle.load(input_file)\n    input_file.close()\n    \nwith open(root + 'train_undersampled.pkl', 'rb') as input_file:\n    train_all_tokenized = pickle.load(input_file)\n    input_file.close()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:23:10.357728Z","iopub.execute_input":"2022-05-09T13:23:10.358158Z","iopub.status.idle":"2022-05-09T13:23:13.621866Z","shell.execute_reply.started":"2022-05-09T13:23:10.358116Z","shell.execute_reply":"2022-05-09T13:23:13.620987Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# train_tokenized_small = train_all_tokenized.shuffle(seed=global_seed).select(range(1000))\n# test_tokenized_small = test_all_tokenized.shuffle(seed=global_seed).select(range(100))\n\n# train_tokenized = train_tokenized_small.remove_columns(['id'])\n# test_tokenized = test_tokenized_small.remove_columns(['id'])\n\n# only keep int/float columns\ntrain_tokenized = train_all_tokenized.remove_columns(['id'])\ntest_tokenized = test_all_tokenized.remove_columns(['id'])","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:23:13.624126Z","iopub.execute_input":"2022-05-09T13:23:13.624714Z","iopub.status.idle":"2022-05-09T13:23:13.635726Z","shell.execute_reply.started":"2022-05-09T13:23:13.624671Z","shell.execute_reply":"2022-05-09T13:23:13.634957Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nBERT (Bidirectional Encoder Representations from Transformers) is a state of the art approach to various NLP tasks. It uses a Transformer architecture and relies heavily on the concept of pre-training.\n\nWe'll use a pre-trained BERT-base model, provided in huggingface [transformers](https://github.com/huggingface/transformers) repo. It gives us a pytorch implementation for the classic BERT architecture, as well as a tokenizer and weights pre-trained on a public English corpus (Wikipedia).\n\nPlease follow these [installation instrucitons](https://github.com/huggingface/transformers#installation) before proceeding.","metadata":{"id":"smey0wVLOG3Q"}},{"cell_type":"code","source":"# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForSequenceClassification\nfrom transformers import AutoModelForSequenceClassification\n\ndef load_pretrained_model(model_name, num_labels):\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n    trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n    total_params = 0\n    trainable_params = 0\n\n    for p in model.parameters():\n        p.requires_grad = False\n        total_params += p.numel()\n\n    for layer in trainable_layers:\n        for p in layer.parameters():\n            p.requires_grad = True\n            trainable_params += p.numel()\n\n    print(f\"Total parameters count: {total_params}\") # ~108M\n    print(f\"Trainable parameters count: {trainable_params}\") # ~7M\n\n    return model","metadata":{"id":"XtuQ-VCYOJM_","execution":{"iopub.status.busy":"2022-05-09T13:23:13.639466Z","iopub.execute_input":"2022-05-09T13:23:13.639662Z","iopub.status.idle":"2022-05-09T13:23:13.646966Z","shell.execute_reply.started":"2022-05-09T13:23:13.639638Z","shell.execute_reply":"2022-05-09T13:23:13.646220Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"num_labels = 2\n# model_name = \"bert-base-uncased\"\nmodel_name = 'prajjwal1/bert-small' # https://huggingface.co/prajjwal1/bert-small","metadata":{"id":"dfJ3Mun0L4uR","outputId":"5891754e-9c58-41a3-ec1a-64fddca917d1","execution":{"iopub.status.busy":"2022-05-09T13:23:13.648347Z","iopub.execute_input":"2022-05-09T13:23:13.649102Z","iopub.status.idle":"2022-05-09T13:23:13.662925Z","shell.execute_reply.started":"2022-05-09T13:23:13.649057Z","shell.execute_reply":"2022-05-09T13:23:13.662035Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Private Training","metadata":{"id":"Cf0mMuDWNQla"}},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nsigmoid = torch.nn.Sigmoid()\n\n# https://huggingface.co/docs/datasets/metrics\ndef calculate_result(labels, probs, threshold=0.5):\n    preds = np.where(probs >= threshold, 1, 0)\n    return {\n        'accuracy': np.round(accuracy_score(labels, preds), 4),\n        'f1': np.round(f1_score(labels, preds), 4),\n        'auc': np.round(roc_auc_score(labels, probs), 4)\n    }\n\ndef evaluate(model, test_dataloader, epoch):    \n    model.eval()\n\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n    progress_bar = tqdm(range(len(test_dataloader)), desc=f'Epoch {epoch} (Test)')\n    \n    for batch in test_dataloader:\n        inputs = {k: v.to(device) for k, v in batch.items()}\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        \n        probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n        labels = inputs[target].detach().cpu().numpy()\n        \n        losses.append(loss.item())\n        total_probs = torch.cat((total_probs, probs), dim=0)\n        total_labels.extend(labels)\n        \n        progress_bar.update(1)\n        progress_bar.set_postfix(\n            loss=np.round(np.mean(losses), 4), \n            f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n        )\n    \n    model.train()\n    test_result = calculate_result(total_labels, total_probs)\n    return np.mean(losses), test_result, total_probs\n\ndef dp_train(model, train_dataloader, epoch):\n    losses, total_labels = [], []\n    total_probs = torch.tensor([], dtype=torch.float32)\n\n    with BatchMemoryManager(\n        data_loader=train_dataloader, \n        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n        optimizer=optimizer\n    ) as memory_safe_data_loader:\n        progress_bar = tqdm(range(len(memory_safe_data_loader)), desc=f'Epoch {epoch} (Train)')\n\n        for step, data in enumerate(memory_safe_data_loader):\n            optimizer.zero_grad()\n\n            inputs = {k: v.to(device) for k, v in data.items()}\n            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n\n            # loss = loss_function(outputs.logits, targets)\n            loss = outputs[0]\n\n            loss.backward()\n            optimizer.step()\n\n            losses.append(loss.item())\n\n            # preds = np.argmax(outputs.logits.detach().cpu().numpy(), axis=1)\n            probs = sigmoid(outputs.logits.detach().cpu())[:, 1]\n            labels = inputs[target].detach().cpu().numpy()\n            \n            total_probs = torch.cat((total_probs, probs), dim=0)\n            total_labels.extend(labels)\n\n            progress_bar.update(1)\n            progress_bar.set_postfix(\n                loss=np.round(np.mean(losses), 4), \n                f1=np.round(f1_score(total_labels, total_probs>=0.5), 4)\n            )\n\n    train_loss = np.mean(losses)\n    train_result = calculate_result(np.array(total_labels), np.array(total_probs))\n\n    return train_loss, train_result, total_probs\n\ndef dump_results(epoch=None):\n    n_train, n_test = len(train_probs), len(test_probs)\n    train_df = pd.DataFrame({\n        'id':train_all_tokenized['id'][:n_train], 'labels':train_all_tokenized[target][:n_train], \n        'probs': train_probs, 'split':['train']* n_train\n    })\n    test_df = pd.DataFrame({\n        'id':test_all_tokenized['id'][:n_test], 'labels':test_all_tokenized[target][:n_test], \n        'probs': test_probs, 'split':['test']* n_test\n    })\n\n    total_df = pd.concat([train_df, test_df],ignore_index=True)\n\n    if epoch is None:\n        total_df.to_csv('results_dp.csv', index=False)\n    else:\n        total_df.to_csv(f'results_dp_{epoch}.csv', index=False)","metadata":{"id":"ae5nsaxPy6LC","execution":{"iopub.status.busy":"2022-05-09T13:23:13.664489Z","iopub.execute_input":"2022-05-09T13:23:13.665141Z","iopub.status.idle":"2022-05-09T13:23:14.017096Z","shell.execute_reply.started":"2022-05-09T13:23:13.665100Z","shell.execute_reply":"2022-05-09T13:23:14.016379Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Data loader\n\n[How to choose batch size in DP](https://github.com/pytorch/opacus/blob/main/tutorials/building_text_classifier.ipynb)","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\n\n# needed for DP training\nMAX_PHYSICAL_BATCH_SIZE = 64\n\ntrain_dataloader = DataLoader(train_tokenized, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_tokenized, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T13:23:14.018397Z","iopub.execute_input":"2022-05-09T13:23:14.018659Z","iopub.status.idle":"2022-05-09T13:23:14.023025Z","shell.execute_reply.started":"2022-05-09T13:23:14.018616Z","shell.execute_reply":"2022-05-09T13:23:14.022356Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Model and optimizer","metadata":{}},{"cell_type":"code","source":"EPOCHS = 1\ndelta_list = [5e-2, 1e-3, 1e-5]\nNOISE_MULTIPLIER = 0.35\nLEARNING_RATE = 1e-3\nMAX_GRAD_NORM = 1","metadata":{"id":"J55PhL-SNXIy","execution":{"iopub.status.busy":"2022-05-09T13:23:14.024423Z","iopub.execute_input":"2022-05-09T13:23:14.024826Z","iopub.status.idle":"2022-05-09T13:23:14.033168Z","shell.execute_reply.started":"2022-05-09T13:23:14.024789Z","shell.execute_reply":"2022-05-09T13:23:14.032503Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# load a fresh model each time\nmodel = load_pretrained_model(model_name, num_labels)\n\n# Set the model to train mode (HuggingFace models load in eval mode)\nmodel = model.train().to(device)\n\n# Define optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n\n# https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)","metadata":{"id":"2VJx_xjQNTaZ","execution":{"iopub.status.busy":"2022-05-09T13:23:14.034563Z","iopub.execute_input":"2022-05-09T13:23:14.034826Z","iopub.status.idle":"2022-05-09T13:23:26.581846Z","shell.execute_reply.started":"2022-05-09T13:23:14.034791Z","shell.execute_reply":"2022-05-09T13:23:26.581125Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Privacy Engine","metadata":{"id":"b7EFOaY2NeiR"}},{"cell_type":"code","source":"from opacus import PrivacyEngine\n\nprivacy_engine = PrivacyEngine()","metadata":{"id":"R00MPKNsh205","execution":{"iopub.status.busy":"2022-05-09T13:23:26.587315Z","iopub.execute_input":"2022-05-09T13:23:26.589825Z","iopub.status.idle":"2022-05-09T13:23:26.596008Z","shell.execute_reply.started":"2022-05-09T13:23:26.589759Z","shell.execute_reply":"2022-05-09T13:23:26.595278Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# using this method decreases how many batches BatchMemoryManager can load\n# e.g. with 1000 examples, 128 batch size, 64 max physical batch size, using BatchMemoryManager \n# with train_dataloader before applying this function returns length 16. But applying this method \n# on, same train_dataloader makes a BatchMemoryManager of length 15 instead.\n# EPSILON = 10\n# # DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarant\n# DELTA = 5e-2\n\n# model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n#     module=model,\n#     optimizer=optimizer,\n#     data_loader=train_dataloader,\n#     target_delta=DELTA,\n#     target_epsilon=EPSILON, \n#     epochs=EPOCHS,\n#     max_grad_norm=MAX_GRAD_NORM,\n# )\n\nmodel, optimizer, train_dataloader = privacy_engine.make_private(\n    module=model,\n    optimizer=optimizer,\n    data_loader=train_dataloader,\n    noise_multiplier=NOISE_MULTIPLIER,\n    max_grad_norm=MAX_GRAD_NORM,\n    poisson_sampling=False,\n)","metadata":{"id":"Wta4qx6mNgqB","execution":{"iopub.status.busy":"2022-05-09T13:23:26.600679Z","iopub.execute_input":"2022-05-09T13:23:26.603068Z","iopub.status.idle":"2022-05-09T13:23:26.618418Z","shell.execute_reply.started":"2022-05-09T13:23:26.603031Z","shell.execute_reply":"2022-05-09T13:23:26.617557Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Loop","metadata":{}},{"cell_type":"code","source":"for epoch in range(2, EPOCHS+3):\n    gc.collect()\n    \n    train_loss, train_result, train_probs = dp_train(model, train_dataloader, epoch)\n    test_loss, test_result, test_probs = evaluate(model, test_dataloader, epoch)\n    \n    epsilons = []\n    for delta in delta_list:\n        epsilons.append(privacy_engine.get_epsilon(delta))\n\n    print(\n      f\"Epoch: {epoch} | \"\n      f\"ɛ: {np.round(epsilons, 2)} |\"\n      f\"Train loss: {train_loss:.3f} | \"\n      f\"Train result: {train_result} |\\n\"\n      f\"Test loss: {test_loss:.3f} | \"\n      f\"Test result: {test_result} | \"\n    )\n    \n    dump_results(epoch)\n    lr_scheduler.step()","metadata":{"id":"RkZPY4iIUUOw","outputId":"aebf8df4-0009-4678-b0d5-392753e178e9","execution":{"iopub.status.busy":"2022-05-09T13:44:50.938571Z","iopub.execute_input":"2022-05-09T13:44:50.939276Z","iopub.status.idle":"2022-05-09T14:25:59.339259Z","shell.execute_reply.started":"2022-05-09T13:44:50.939238Z","shell.execute_reply":"2022-05-09T14:25:59.338498Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Noise multiplier 0.35\nEpoch 1 (Train): 100%\n4511/4512 [19:44<00:00, 2.75it/s, f1=0.802, loss=0.807]\nEpoch 1 (Test): 100%\n1521/1521 [04:47<00:00, 3.28it/s, f1=0.426, loss=0.781]\nEpoch: 1 | ɛ: [ 2.95  7.08 11.52] |Train loss: 0.808 | Train result: {'accuracy': 0.8002, 'f1': 0.8025, 'auc': 0.8809} |\nTest loss: 0.781 | Test result: {'accuracy': 0.8169, 'f1': 0.4261, 'auc': 0.9093} | \n\nEpoch 2 (Train): 100%\n4511/4512 [19:43<00:00, 2.89it/s, f1=0.825, loss=0.778]\nEpoch 2 (Test): 100%\n1521/1521 [04:49<00:00, 4.25it/s, f1=0.451, loss=0.743]\nEpoch: 2 | ɛ: [ 3.78  8.38 13.24] |Train loss: 0.778 | Train result: {'accuracy': 0.8232, 'f1': 0.8253, 'auc': 0.8913} |\nTest loss: 0.743 | Test result: {'accuracy': 0.8361, 'f1': 0.4513, 'auc': 0.9066} | \n\nEpoch 3 (Train): 100%\n4511/4512 [19:42<00:00, 3.30it/s, f1=0.828, loss=0.776]\nEpoch 3 (Test): 100%\n1521/1521 [04:46<00:00, 4.13it/s, f1=0.414, loss=0.856]\nEpoch: 3 | ɛ: [ 4.44  9.46 14.57] |Train loss: 0.776 | Train result: {'accuracy': 0.8267, 'f1': 0.8281, 'auc': 0.8917} |\nTest loss: 0.856 | Test result: {'accuracy': 0.8018, 'f1': 0.4142, 'auc': 0.9043} | \n\n### Noise multipler 0.5\n\nEpoch 1 (Train): 100%\n4511/4512 [19:50<00:00, 3.38it/s, f1=0.78, loss=0.879]\nEpoch 1 (Test): 100%\n1521/1521 [04:46<00:00, 4.30it/s, f1=0.445, loss=0.78]\nEpoch: 1 | ɛ: [0.48 1.98 3.66] |Train loss: 0.879 | Train result: {'accuracy': 0.7828, 'f1': 0.78, 'auc': 0.862} |\nTest loss: 0.780 | Test result: {'accuracy': 0.8386, 'f1': 0.4449, 'auc': 0.8977} | \n\nEpoch 2 (Train): 100%\n4511/4512 [20:06<00:00, 3.30it/s, f1=0.81, loss=0.842]\nEpoch 2 (Test): 100%\n1521/1521 [04:52<00:00, 4.17it/s, f1=0.44, loss=0.899]\nEpoch: 2 | ɛ: [0.58 2.15 3.9 ] |Train loss: 0.842 | Train result: {'accuracy': 0.8136, 'f1': 0.8101, 'auc': 0.8782} |\nTest loss: 0.899 | Test result: {'accuracy': 0.8291, 'f1': 0.4395, 'auc': 0.9005} | \n\n\nEpoch 3 (Train): 100%\n4511/4512 [20:09<00:00, 3.26it/s, f1=0.815, loss=0.832]\nEpoch 3 (Test): 100%\n1521/1521 [04:53<00:00, 4.13it/s, f1=0.42, loss=0.93]\nEpoch: 3 | ɛ: [0.66 2.28 4.08] |Train loss: 0.832 | Train result: {'accuracy': 0.8168, 'f1': 0.8148, 'auc': 0.8785} |\nTest loss: 0.930 | Test result: {'accuracy': 0.8111, 'f1': 0.4199, 'auc': 0.8953} | \n\n\n### Noise Multiplier 0.4\nEpoch 2 (Train): 100%\n4511/4512 [20:03<00:00, 3.33it/s, f1=0.786, loss=0.867]\nEpoch 2 (Test): 100%\n1521/1521 [04:52<00:00, 4.23it/s, f1=0.424, loss=0.859]\nEpoch: 2 | ɛ: [1.64 4.47 7.54] |Train loss: 0.867 | Train result: {'accuracy': 0.7878, 'f1': 0.7857, 'auc': 0.8663} |\nTest loss: 0.859 | Test result: {'accuracy': 0.8194, 'f1': 0.4239, 'auc': 0.9017} | \nEpoch 3 (Train): 100%\n4511/4512 [19:59<00:00, 2.70it/s, f1=0.81, loss=0.832]\nEpoch 3 (Test): 100%\n1521/1521 [04:52<00:00, 4.22it/s, f1=0.448, loss=0.816]\nEpoch: 3 | ɛ: [2.02 5.1  8.39] |Train loss: 0.832 | Train result: {'accuracy': 0.8145, 'f1': 0.81, 'auc': 0.8836} |\nTest loss: 0.816 | Test result: {'accuracy': 0.8366, 'f1': 0.4476, 'auc': 0.9058} | \nEpoch 4 (Train): 100%\n4511/4512 [20:03<00:00, 3.21it/s, f1=0.819, loss=0.815]\nEpoch 4 (Test): 100%\n1521/1521 [04:54<00:00, 4.20it/s, f1=0.42, loss=0.902]\nEpoch: 4 | ɛ: [2.32 5.56 9.01] |Train loss: 0.815 | Train result: {'accuracy': 0.8203, 'f1': 0.8191, 'auc': 0.8869} |\nTest loss: 0.902 | Test result: {'accuracy': 0.8101, 'f1': 0.4202, 'auc': 0.9098} | \n","metadata":{}},{"cell_type":"markdown","source":"|Model|Train Time|Learning rate|batch size|epoch |          |  train |         |       | test    |       |Noise multiplier| Epsilon ([5e-2, 1e-3, 1e-5]) |\n|-----|----------|-------------|----------|------|----------|--------|---------|-------|---------|-------|----------------|------------------------------|\n|     |          |             |          |      |  loss    |  f1    |  auc    | loss  | f1      | auc   |                |                              |\n|bert small with lr scheduler|27 min|1e-3|64(32)|1| 0.793   | 0.8183 |  0.8776 | 1.016 |  0.3964 | 0.9017|     0.1        |   1058.57, 1097.69, 1143.74  |\n|     | 25min    |      1e-3   |          |   2  |  0.766   | 0.8297 |  0.8741 | 0.794 |  0.4338 | 0.8961|     0.1        |   2090.53, 2129.65, 2175.7   |\n|bert small with lr scheduler|17 min|5e-4|32(32)|1| 0.774   | 0.815  |  0.8937 | 0.774 |  0.4765 | 0.9222|     0.1        |   1144.95, 1184.07, 1230.12  |\n|     | 21min    |      5e-4   |          |   2  |  0.736   | 0.8375 |  0.9067 | 0.736 |  0.4739 | 0.9259|     0.1        |   2237.59, 2276.71, 2322.77  |\n|     | 17min    |      5e-4   |          |   3  |  0.737   | 0.8325 |  0.8976 | 0.788 |  0.4352 | 0.9174|     0.1        |   3330.24, 3369.36, 3415.41  |\n|bert small with lr scheduler|21 min|1e-3|128(64)|1| 0.774  | 0.8204 |  0.8937 | 0.774 |  0.4765 | 0.9222|     0.1        |   1144.95, 1184.07, 1230.12  |\n|     |          |      1e-3   |          |   2  |  0.736   | 0.8375 |  0.9067 | 0.736 |  0.4739 | 0.9259|     0.1        |   2237.59, 2276.71, 2322.77  |\n|     |          |      1e-3   |          |   3  |  0.737   | 0.8325 |  0.8976 | 0.788 |  0.4352 | 0.9174|     0.1        |   3330.24, 3369.36, 3415.41  |\n|bert small with lr scheduler|20 min|1e-3|128(64)|1| 0.779  | 0.8086 |  0.8885 | 0.689 |  0.4790 | 0.9161|     0.2        |     28.38, 47.94, 70.96      |\n|     | 21min    |      1e-3   |          |   2  |  0.758   | 0.8282 |  0.8999 | 0.689 |  0.4750 | 0.9179|     0.2        |     42.57, 64.04, 87.06      |\n|     | 17min    |      1e-3   |          |   3  |  0.746   | 0.8318 |  0.8984 | 0.696 |  0.4733 | 0.9175|     0.2        |     50.56, 80.14, 103.16     |\n|     | 20 min   |      1e-3   |  128(64) |   1  |  0.844   | 0.8006 |  0.8721 | 0.722 |  0.4650 | 0.9006|     0.25       |     12.38, 23.28, 34.79      |\n|     |          |      1e-3   |          |   2  |  0.797   | 0.8236 |  0.8831 | 0.707 |  0.4726 | 0.9026|     0.25       |     17.11, 30.15, 42.89      |\n|     |          |      1e-3   |          |   3  |  0.783   | 0.8237 |  0.8760 | 0.777 |  0.4749 | 0.8934|     0.25       |     21.32, 34.89, 50.24      |\n|     | 22min    |      1e-3   |  128(64) |   1  |  0.794   | 0.7977 |  0.8814 | 0.698 |  0.4713 | 0.9025|     0.3        |     51.14, 80.88, 103.91     |\n|     |          |      1e-3   |          |   2  |  0.773   | 0.8229 |  0.8894 | 0.818 |  0.4455 | 0.9055|     0.3        |     51.72, 81.63, 104.65     |\n|     |          |      1e-3   |          |   3  |  0.770   | 0.8269 |  0.8864 | 0.801 |  0.4445 | 0.9018|     0.3        |     52.30, 82.37, 105.40     |","metadata":{}},{"cell_type":"markdown","source":"## Save model\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html","metadata":{}},{"cell_type":"code","source":"model_path = 'dp_model_state_dict.pt'\ntorch.save(model.state_dict(), model_path)\n\n# model.load_state_dict(torch.load(model_path))\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:27:27.369872Z","iopub.execute_input":"2022-05-09T14:27:27.370124Z","iopub.status.idle":"2022-05-09T14:27:27.698884Z","shell.execute_reply.started":"2022-05-09T14:27:27.370096Z","shell.execute_reply":"2022-05-09T14:27:27.697932Z"},"trusted":true},"execution_count":22,"outputs":[]}]}